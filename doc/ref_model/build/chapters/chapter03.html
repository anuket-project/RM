<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Modelling &#8212; Anuket Reference Model (RM)  documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
    <link rel="stylesheet" type="text/css" href="../_static/basic_mod.css?v=db1dbc67" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=85408339" />
    <script src="../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <script src="../_static/js/petite-vue.js"></script>
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
  </head><body data-dark_mode_code_blocks="true">

<div id="top_nav">
    

    <nav>
        
            
        

        <p id="toggle_sidebar">
            <a href="#" title="Toggle sidebar">|||</a>
        </p>
        <h1><a href="../index.html" title="Go to homepage"><img src="../_static/anuket-logo.png"/></a></h1>

        <a id="mode_toggle" href="#" @click.prevent="handleClick" :title="mode">
    <template v-if="mode == 'light'">
        <svg width="100%" height="100%" viewBox="0 0 79 80" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;"><g id="mode_light"><rect id="Bounds" x="0" y="-0" width="78.623" height="79.049" style="fill:none;"/><circle cx="39.311" cy="39.524" r="15.734" style="fill:#fff;"/><g id="beams"><g id="beam"><path id="beam1" serif:id="beam" d="M44.212,4.901c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.9,2.196 -4.9,4.901l-0,9.614c-0,2.705 2.196,4.901 4.9,4.901c2.705,0 4.901,-2.196 4.901,-4.901l0,-9.614Z" style="fill:#fff;"/></g><g id="beam2" serif:id="beam"><path id="beam3" serif:id="beam" d="M67.48,18.073c1.913,-1.912 1.913,-5.018 0,-6.931c-1.912,-1.912 -5.018,-1.912 -6.931,0l-6.798,6.799c-1.912,1.912 -1.912,5.018 0,6.931c1.913,1.912 5.018,1.912 6.931,-0l6.798,-6.799Z" style="fill:#fff;"/></g><g id="beam4" serif:id="beam"><path id="beam5" serif:id="beam" d="M25.728,61.108c1.912,-1.913 1.912,-5.018 -0,-6.931c-1.913,-1.913 -5.019,-1.913 -6.931,-0l-6.799,6.798c-1.912,1.913 -1.912,5.019 0,6.931c1.913,1.913 5.019,1.913 6.931,0l6.799,-6.798Z" style="fill:#fff;"/></g><g id="beam6" serif:id="beam"><path id="beam7" serif:id="beam" d="M60.682,54.177c-1.913,-1.913 -5.018,-1.913 -6.931,-0c-1.912,1.913 -1.912,5.018 0,6.931l6.798,6.798c1.913,1.913 5.019,1.913 6.931,0c1.913,-1.912 1.913,-5.018 0,-6.931l-6.798,-6.798Z" style="fill:#fff;"/></g><g id="beam8" serif:id="beam"><path id="beam9" serif:id="beam" d="M4.901,34.623c-2.705,0 -4.901,2.196 -4.901,4.901c0,2.705 2.196,4.901 4.901,4.901l9.614,0c2.705,0 4.901,-2.196 4.901,-4.901c0,-2.705 -2.196,-4.901 -4.901,-4.901l-9.614,0Z" style="fill:#fff;"/></g><g id="beam10" serif:id="beam"><path id="beam11" serif:id="beam" d="M44.212,64.534c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.9,2.196 -4.9,4.901l-0,9.614c-0,2.705 2.196,4.901 4.9,4.901c2.705,-0 4.901,-2.196 4.901,-4.901l0,-9.614Z" style="fill:#fff;"/></g><g id="beam12" serif:id="beam"><path id="beam13" serif:id="beam" d="M18.929,11.142c-1.912,-1.912 -5.018,-1.912 -6.931,0c-1.912,1.913 -1.912,5.019 0,6.931l6.799,6.799c1.912,1.912 5.018,1.912 6.931,-0c1.912,-1.913 1.912,-5.019 -0,-6.931l-6.799,-6.799Z" style="fill:#fff;"/></g><g id="beam14" serif:id="beam"><path id="beam15" serif:id="beam" d="M64.108,34.623c-2.705,0 -4.901,2.196 -4.901,4.901c-0,2.705 2.196,4.901 4.901,4.901l9.614,0c2.705,0 4.901,-2.196 4.901,-4.901c-0,-2.705 -2.196,-4.901 -4.901,-4.901l-9.614,0Z" style="fill:#fff;"/></g></g></g></svg>
    </template>

    <template v-if="mode == 'dark'">
        <svg width="100%" height="100%" viewBox="0 0 79 80" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;"><g id="mode_dark"><rect id="Bounds" x="0" y="-0" width="78.623" height="79.049" style="fill:none;"/><circle cx="39.311" cy="39.524" r="15.734" style="fill:#fff;"/><g id="beams"><g id="beam"><path id="beam1" serif:id="beam" d="M44.212,14.515c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,0 -4.901,2.196 -4.901,4.901c0,2.705 2.197,4.901 4.901,4.901c2.705,0 4.901,-2.196 4.901,-4.901Z" style="fill:#fff;"/></g><g id="beam2" serif:id="beam"><path id="beam3" serif:id="beam" d="M60.662,24.892c1.902,-1.902 1.902,-4.99 0,-6.892l-0.04,-0.039c-1.901,-1.902 -4.989,-1.902 -6.891,-0c-1.901,1.901 -1.901,4.989 0,6.891l0.04,0.04c1.902,1.901 4.989,1.901 6.891,-0Z" style="fill:#fff;"/></g><g id="beam4" serif:id="beam"><path id="beam5" serif:id="beam" d="M25.732,61.103c1.91,-1.91 1.91,-5.011 0,-6.921l-0.009,-0.01c-1.91,-1.91 -5.012,-1.91 -6.921,-0c-1.91,1.91 -1.91,5.011 -0,6.921l0.01,0.01c1.909,1.91 5.011,1.91 6.92,-0Z" style="fill:#fff;"/></g><g id="beam6" serif:id="beam"><path id="beam7" serif:id="beam" d="M60.672,54.167c-1.907,-1.907 -5.004,-1.907 -6.911,0l-0.02,0.02c-1.907,1.907 -1.907,5.004 0,6.911c1.907,1.907 5.004,1.907 6.911,-0l0.02,-0.02c1.907,-1.907 1.907,-5.004 0,-6.911Z" style="fill:#fff;"/></g><g id="beam8" serif:id="beam"><path id="beam9" serif:id="beam" d="M14.52,34.623c-2.702,0 -4.896,2.194 -4.896,4.896l0,0.01c0,2.702 2.194,4.896 4.896,4.896c2.702,0 4.896,-2.194 4.896,-4.896l-0,-0.01c-0,-2.702 -2.194,-4.896 -4.896,-4.896Z" style="fill:#fff;"/></g><g id="beam10" serif:id="beam"><path id="beam11" serif:id="beam" d="M44.212,64.534c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.901,2.196 -4.901,4.901c0,2.704 2.197,4.9 4.901,4.9c2.705,0 4.901,-2.196 4.901,-4.9Z" style="fill:#fff;"/></g><g id="beam12" serif:id="beam"><path id="beam13" serif:id="beam" d="M25.73,17.943c-1.911,-1.911 -5.015,-1.911 -6.926,0l-0.005,0.005c-1.911,1.911 -1.911,5.015 0,6.926c1.911,1.911 5.015,1.911 6.926,0l0.005,-0.005c1.911,-1.911 1.911,-5.014 -0,-6.926Z" style="fill:#fff;"/></g><g id="beam14" serif:id="beam"><path id="beam15" serif:id="beam" d="M64.098,34.623c-2.699,0 -4.891,2.192 -4.891,4.892l-0,0.019c-0,2.699 2.192,4.891 4.891,4.891c2.7,0 4.892,-2.192 4.892,-4.891l0,-0.019c0,-2.7 -2.192,-4.892 -4.892,-4.892Z" style="fill:#fff;"/></g></g></g></svg>
    </template>

    <template v-if="mode == 'darkest'">
        <svg width="100%" height="100%" viewBox="0 0 79 80" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;"><g id="mode_darkest"><rect id="Bounds" x="0" y="-0" width="78.623" height="79.049" style="fill:none;"/><path d="M39.315,23.791c8.684,-0 15.734,7.05 15.734,15.733c0,8.684 -7.05,15.734 -15.734,15.734c-8.683,0 -15.733,-7.05 -15.733,-15.734c-0,-8.683 7.05,-15.733 15.733,-15.733Zm0,4.737c6.069,0 10.997,4.927 10.997,10.996c-0,6.069 -4.928,10.996 -10.997,10.996c-6.068,0 -10.996,-4.927 -10.996,-10.996c0,-6.069 4.928,-10.996 10.996,-10.996Z" style="fill:#fff;"/><g id="beams"><g id="beam"><path id="beam1" serif:id="beam" d="M44.216,14.515c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,0 -4.9,2.196 -4.9,4.901c-0,2.705 2.196,4.901 4.9,4.901c2.705,0 4.901,-2.196 4.901,-4.901Z" style="fill:#fff;"/></g><g id="beam2" serif:id="beam"><path id="beam3" serif:id="beam" d="M60.666,24.892c1.902,-1.902 1.902,-4.99 0,-6.892l-0.04,-0.039c-1.901,-1.902 -4.989,-1.902 -6.891,-0c-1.901,1.901 -1.901,4.989 0,6.891l0.04,0.04c1.902,1.901 4.99,1.901 6.891,-0Z" style="fill:#fff;"/></g><g id="beam4" serif:id="beam"><path id="beam5" serif:id="beam" d="M25.737,61.103c1.909,-1.91 1.909,-5.011 -0,-6.921l-0.01,-0.01c-1.91,-1.91 -5.011,-1.91 -6.921,-0c-1.91,1.91 -1.91,5.011 -0,6.921l0.01,0.01c1.91,1.91 5.011,1.91 6.921,-0Z" style="fill:#fff;"/></g><g id="beam6" serif:id="beam"><path id="beam7" serif:id="beam" d="M60.676,54.167c-1.907,-1.907 -5.004,-1.907 -6.911,0l-0.02,0.02c-1.907,1.907 -1.907,5.004 0,6.911c1.907,1.907 5.004,1.907 6.911,-0l0.02,-0.02c1.907,-1.907 1.907,-5.004 0,-6.911Z" style="fill:#fff;"/></g><g id="beam8" serif:id="beam"><path id="beam9" serif:id="beam" d="M14.524,34.623c-2.702,0 -4.896,2.194 -4.896,4.896l0,0.01c0,2.702 2.194,4.896 4.896,4.896c2.702,0 4.896,-2.194 4.896,-4.896l0,-0.01c0,-2.702 -2.194,-4.896 -4.896,-4.896Z" style="fill:#fff;"/></g><g id="beam10" serif:id="beam"><path id="beam11" serif:id="beam" d="M44.216,64.534c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.9,2.196 -4.9,4.901c-0,2.704 2.196,4.9 4.9,4.9c2.705,0 4.901,-2.196 4.901,-4.9Z" style="fill:#fff;"/></g><g id="beam12" serif:id="beam"><path id="beam13" serif:id="beam" d="M25.734,17.943c-1.911,-1.911 -5.015,-1.911 -6.926,0l-0.005,0.005c-1.911,1.911 -1.911,5.015 0,6.926c1.911,1.911 5.015,1.911 6.926,0l0.005,-0.005c1.911,-1.911 1.911,-5.014 0,-6.926Z" style="fill:#fff;"/></g><g id="beam14" serif:id="beam"><path id="beam15" serif:id="beam" d="M64.103,34.623c-2.7,0 -4.892,2.192 -4.892,4.892l-0,0.019c-0,2.699 2.192,4.891 4.892,4.891c2.699,0 4.891,-2.192 4.891,-4.891l0,-0.019c0,-2.7 -2.192,-4.892 -4.891,-4.892Z" style="fill:#fff;"/></g></g></g></svg>
    </template>
</a>

<script>
(function() {
    const LOCAL_STORAGE_KEY = 'piccoloThemeMode'

    var initialMode = localStorage.getItem(LOCAL_STORAGE_KEY)

    if (initialMode) {
        // Make sure the value in local storage is valid
        if (['light', 'dark', 'darkest'].indexOf(initialMode) == -1) {
            initialMode = 'light'
            localStorage.setItem(LOCAL_STORAGE_KEY, initialMode)
        }
    } else {
        // Check if the client prefers dark mode
        if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
            initialMode = 'dark'
        } else {
            initialMode = 'light'
        }
        localStorage.setItem(LOCAL_STORAGE_KEY, initialMode)
    }

    document.documentElement.dataset.mode = initialMode

    PetiteVue.createApp({
        'mode': initialMode,
        handleClick() {
            let currentMode = this.mode

            if (currentMode == 'light') {
                this.mode = 'dark'
            } else if (currentMode == 'dark') {
                this.mode = 'darkest'
            } else if (currentMode == 'darkest') {
                this.mode = 'light'
            }

            document.documentElement.dataset.mode = this.mode
            localStorage.setItem(LOCAL_STORAGE_KEY, this.mode)

            console.log(this.mode)
        }
    }).mount('#mode_toggle')
})()
</script>
            <p class="mobile_search_link">
                <a href="../search.html" title="Search">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 65 64" fill-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="2">
                        <path d="M14.873 40.009c-2.315-3.943-3.642-8.532-3.642-13.429C11.231 11.91 23.141 0 37.811 0s26.58 11.91 26.58 26.58-11.91 26.58-26.58 26.58a26.44 26.44 0 0 1-14.277-4.161L9.739 62.794a3.12 3.12 0 0 1-4.413 0L.913 58.382c-1.217-1.218-1.217-3.196 0-4.413l13.96-13.96zM37.811 8.054c10.225 0 18.526 8.301 18.526 18.526s-8.301 18.526-18.526 18.526-18.526-8.301-18.526-18.526S27.586 8.054 37.811 8.054z" fill="#fff" />
                    </svg>
                </a>
            </p>
        

        <div class="searchbox_wrapper">
            
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
    </nav>
</div>

    
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
        </div>
      </div>


    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="modelling">
<h1>Modelling<a class="headerlink" href="#modelling" title="Link to this heading">¶</a></h1>
<p>It is necessary to clearly define the infrastructure resources and their capabilities that a shared cloud infrastructure
(network function virtualisation infrastructure, NFVI) provides for hosting workloads including virtual network
functions (VNFs) and/or cloud-native network functions (CNFs). A common understanding of which resources and their
corresponding capabilities a cloud infrastructure provides or shall provide will help improve workload onboarding
efficiency and avoid issues that could negatively impact the time and the cost of onboarding and maintaining target
workloads and solutions on top of a virtualised infrastructure.</p>
<p>The abstraction model presented in this Reference Model (RM) specifies a common set of virtual infrastructure resources
that a cloud infrastructure will need to provide to be able to host most of the typical VNF/CNF telco workloads. The
intention of this Reference Model is to follow the following principles:</p>
<ul class="simple">
<li><p><strong>Scope:</strong> the model should describe the most relevant virtualised infrastructure resources (incl. acceleration
technologies) a cloud infrastructure needs to host Telco workloads</p></li>
<li><p><strong>Separation of Concern:</strong> the model should support a clear distinction between the responsibilities related to
maintaining the network function virtualisation infrastructure and the responsibilities related to managing the
various VNF workloads</p></li>
<li><p><strong>Simplicity:</strong> the amount of different types of resources (including their attributes and relationships amongst one
another) should be kept to a minimum to reduce the configuration spectrum which needs to be considered</p></li>
<li><p><strong>Declarative</strong>: the model should allow for the description of the intended state and configuration of the cloud
infrastructure resources for automated life cycle management</p></li>
<li><p><strong>Explicit:</strong> the model needs to be rich enough to cover the instantiation and the on-going operation of the cloud
infrastructure</p></li>
<li><p><strong>Lifecycle:</strong> the model must distinguish between resources which have independent lifecycles but should group
together those resources which share a common lifecycle</p></li>
<li><p><strong>Aligned:</strong> the model should clearly highlight the dependencies between its components to allow for a well-defined
and simplified synchronisation of independent automation tasks.</p></li>
</ul>
<p><strong>To summarise:</strong><em>the abstraction model presented in this document will build upon existing modelling concepts and
simplify and streamline them to the needs of telco operators who intend to distinguish between infrastructure related
and workload related responsibilities.</em></p>
<section id="model">
<h2>Model<a class="headerlink" href="#model" title="Link to this heading">¶</a></h2>
<p>The abstraction model for the cloud infrastructure is divided into two logical layers: the virtual infrastructure layer
and the hardware infrastructure layer, with the intention that only the virtual infrastructure layer will be directly
exposed to workloads (VNFs/CNFs):</p>
<figure class="align-default" id="cloud-infrastructure-model-overview">
<img alt="Cloud Infrastructure Model Overview" src="../_images/ch03-model-overview.png" />
<figcaption>
<p><span class="caption-text">Cloud Infrastructure Model Overview</span><a class="headerlink" href="#cloud-infrastructure-model-overview" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>The functionalities of each layer are as follows:</p>
<p><strong>Virtual Infrastructure Layer</strong></p>
<ul class="simple">
<li><p><strong>Virtual infrastructure resources:</strong> These are all the infrastructure resources (compute, storage and networks)
which the cloud infrastructure provides to the VNF/CNF and other workloads. These virtual resources can be managed
by the tenants and tenant workloads directly or indirectly via an application programming interface (API).</p></li>
<li><p><strong>Virtual infrastructure manager:</strong> This consists of the software components that manage the virtual resources and
make those management capabilities accessible via one or more APIs. The responsibilities of this functionality include
the management of logical constructs such as tenants, tenant workloads, resource catalogues, identities, access
controls, security policies, etc.</p></li>
</ul>
<p><strong>Hardware Infrastructure Layer</strong></p>
<ul class="simple">
<li><p><strong>Hardware infrastructure manager:</strong> This is a logical block of functionality responsible for the management of the
abstracted hardware resources (compute, network and storage) and as such it is shielded from the direct involvement
with server host software.</p></li>
<li><p><strong>Hardware resources:</strong> These consist of physical hardware components such as servers, (including random access
memory, local storage, network ports, and hardware acceleration devices), storage devices, network devices, and the
basic input output system (BIOS).</p></li>
</ul>
<p><strong>Workload Layer</strong></p>
<ul class="simple">
<li><p><strong>Workloads (VNFs/CNFs):</strong> These consist of workloads such as virtualized and/or containerized network functions that
run within a virtual machine (VM) or as a set of containers.</p></li>
</ul>
</section>
<section id="virtual-infrastructure-layer">
<h2>Virtual Infrastructure Layer<a class="headerlink" href="#virtual-infrastructure-layer" title="Link to this heading">¶</a></h2>
<section id="virtual-resources">
<h3>Virtual Resources<a class="headerlink" href="#virtual-resources" title="Link to this heading">¶</a></h3>
<p>The virtual infrastructure resources provided by the Cloud Infrastructure can be grouped into four categories as shown
in the diagram below:</p>
<figure class="align-default" id="id50">
<img alt="Virtual Infrastructure Resources provide virtual compute, storage and networks in a tenant context" src="../_images/ch03-model-virtual-resources.png" />
<figcaption>
<p><span class="caption-text">Virtual Infrastructure Resources provide virtual compute, storage and networks in a tenant context</span><a class="headerlink" href="#id50" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p><strong>Tenants:</strong> represent an isolated and independently manageable elastic pool of compute, storage and network resources</p></li>
<li><p><strong>Compute resources:</strong> represent virtualised computes for workloads and other systems as necessary</p></li>
<li><p><strong>Storage resources:</strong> represent virtualised resources for persisting data</p></li>
<li><p><strong>Network resources:</strong> represent virtual resources providing layer 2 and layer 3 connectivity</p></li>
</ul>
<p>The virtualised infrastructure resources related to these categories are listed below.</p>
<section id="tenant">
<h4>Tenant<a class="headerlink" href="#tenant" title="Link to this heading">¶</a></h4>
<p>A cloud infrastructure needs to be capable of supporting multiple tenants and has to isolate sets of infrastructure
resources dedicated to specific workloads (VNF/CNF) from one another. Tenants represent an independently manageable
logical pool of compute, storage and network resources abstracted from physical hardware.</p>
<p><strong>Example</strong><em>: a tenant within an OpenStack environment or a Kubernetes cluster.</em></p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Attribute</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">name</span></code></p></td>
<td><p>name of the logical resource pool</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">type</span></code></p></td>
<td><p>type of tenant (e.g. OpenStack tenant, Kubernetes cluster, …)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">vcpus</span></code></p></td>
<td><p>max. number of virtual CPUs</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ram</span></code></p></td>
<td><p>max. size of random access memory in GB</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">disk</span></code></p></td>
<td><p>max. size of ephemeral disk in GB</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">networks</span></code></p></td>
<td><p>description of external networks required for inter-domain connectivity</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">metadata</span></code></p></td>
<td><p>key/value pairs for selection of the appropriate physical context (e.g. location, availability zone, …)</p></td>
</tr>
</tbody>
</table>
<p><strong>Table 3-1:</strong> Attributes of a tenant</p>
</section>
<section id="virtual-compute">
<h4>Virtual Compute<a class="headerlink" href="#virtual-compute" title="Link to this heading">¶</a></h4>
<p>A virtual machine or a container/pod capable of hosting the application components of workloads (VNFs/CNFs) of the
tenant. A virtual compute therefore requires a tenant context and, since it will need to communicate with other
communication partners, it is assumed that the networks have been provisioned in advance.</p>
<p><strong>Example</strong><em>: a virtual compute descriptor as defined in TOSCA Simple Profile for NFV.</em></p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Attribute</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">name</span></code></p></td>
<td><p>name of the virtual host</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">vcpus</span></code></p></td>
<td><p>number of virtual CPUs</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ram</span></code></p></td>
<td><p>size of random access memory in GB</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">disk</span></code></p></td>
<td><p>size of root disc in GB</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">nics</span></code></p></td>
<td><p>sorted list of network interfaces connecting the host to the virtual networks</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">acceleration</span></code></p></td>
<td><p>key/value pairs for selection of the appropriate acceleration technology</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">metadata</span></code></p></td>
<td><p>key/value pairs for selection of the appropriate redundancy domain</p></td>
</tr>
</tbody>
</table>
<p><strong>Table 3-2:</strong> Attributes of compute resources</p>
</section>
<section id="virtual-storage">
<h4>Virtual Storage<a class="headerlink" href="#virtual-storage" title="Link to this heading">¶</a></h4>
<p>A virtual machine and container can consume storage through a number of means. These include storage that is:</p>
<ul class="simple">
<li><p>managed via the hypervisor and container runtime (Hypervisor Attached for virtual machine and Container Persistent for
containers) and is connected via cloud infrastructure underlay network and</p></li>
<li><p>Shared File Storage and the Object storage which is connected via the tenant / user overlay network.
The details of the tenant storage consumption model are covered in section
<a class="reference internal" href="#storage-for-tenant-consumption">Storage for Tenant Consumption</a>.</p></li>
</ul>
<p>In managing the provision of virtual storage the tenant should be able to request alternate performance levels, capacity
and behaviours. The set of selectable attributes includes:</p>
<ul class="simple">
<li><p>Storage class: Block, File, Object.</p></li>
<li><p>Retention Policy - persistent (storage volume / data) is persistent across stop/start of workload; ephemeral storage -
there is no data retention across stop/start events for the workload.</p></li>
<li><p>Underlying physical device type (HDD, SSD, etc.).</p></li>
<li><p>Performance characteristic - defined as: Latency, IOPS (Input/Output Operations per second), and throughput.</p></li>
<li><p>Enhanced features - set of selectable features such as: auto-replicate, encryption, snapshot support.</p></li>
</ul>
<p>Note that approximate numeric ranges for the qualitative values used above are given in the
<a class="reference internal" href="chapter04.html#storage-extensions"><span class="std std-ref">Storage extensions</span></a> section.</p>
<p>Storage resources have the following attributes, with metric definitions that support verification through passive
measurements (telemetry) where appropriate:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Attribute</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">name</span></code></p></td>
<td><p>name of storage resources</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">data</span> <span class="pre">retention</span> <span class="pre">policy</span></code></p></td>
<td><p>persistent or ephemeral</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">performance</span></code></p></td>
<td><p>Read and Write Latency, The average amount of time to perform a R/W operation, in milliseconds</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>Read and Write IOPS, The average rate of performing R/W in IO operations per second</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>Read and Write Throughput, The average rate of performing R/W operations in Bytes per second</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">enhanced</span> <span class="pre">features</span></code></p></td>
<td><p>replication, encryption</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">type</span></code></p></td>
<td><p>block, object or file</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">size</span></code></p></td>
<td><p>size in GB, telemetry includes the amount of free, used, and reserved disk space, in bytes</p></td>
</tr>
</tbody>
</table>
<p><strong>Table 3-3:</strong> Attributes of storage resources</p>
</section>
<section id="virtual-network">
<h4>Virtual Network<a class="headerlink" href="#virtual-network" title="Link to this heading">¶</a></h4>
<p>This topic is covered in <a class="reference external" href="#network">Network</a> section.</p>
</section>
<section id="availability-zone">
<h4>Availability Zone<a class="headerlink" href="#availability-zone" title="Link to this heading">¶</a></h4>
<p>An availability zone is a logical pool of physical resources (e.g. compute, block storage, and network). These logical
pools segment the physical resources of a cloud based on factors chosen by the cloud operator. The cloud operator may
create availability zones based on location (rack, datacenter), or indirect failure domain dependencies like power
sources. Workloads can leverage availability zones to utilise multiple locations or avoid sharing failure domains for a
workload, and thus increase the workloads’ fault-tolerance.</p>
<p>As a logical group with operator-specified criteria, the only mandatory attribute for an Availability Zone is the name.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Attribute</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">name</span></code></p></td>
<td><p>name of the availability zone</p></td>
</tr>
</tbody>
</table>
<p><strong>Table 3-4:</strong> Attributes of availability zones</p>
</section>
</section>
<section id="virtual-infrastructure-manager">
<h3>Virtual Infrastructure Manager<a class="headerlink" href="#virtual-infrastructure-manager" title="Link to this heading">¶</a></h3>
<p>The virtual infrastructure manager allows:</p>
<ul class="simple">
<li><p>setup, manage and delete tenants,</p></li>
<li><p>setup, manage and delete user- and service-accounts,</p></li>
<li><p>manage access privileges and</p></li>
<li><p>provision, manage, monitor and delete virtual resources.</p></li>
</ul>
<figure class="align-default" id="id51">
<img alt="Virtual Infrastructure Manager" src="../_images/ch03-model-virtual-manager.png" />
<figcaption>
<p><span class="caption-text">Virtual Infrastructure Manager</span><a class="headerlink" href="#id51" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>The virtual infrastructure manager needs to support the following functional aspects:</p>
<ul class="simple">
<li><p><strong>API/UI</strong>: an application programming interface / user interface providing access to the virtual resource management
function</p></li>
<li><p><strong>Catalogue</strong>: manages the collection of available templates for virtual resource the cloud infrastructure can provide</p></li>
<li><p><strong>Inventory</strong>: manages the information related to virtual resources of a cloud infrastructure</p></li>
<li><p><strong>Scheduler</strong>: receives requests via API/UI, provisions and manages virtual resources by coordinating the activities
of the compute-, storage- and network resources managers</p></li>
<li><p><strong>Monitoring</strong>: monitors and collects information on all events and the current state of all virtual resources</p></li>
<li><p><strong>Additional Management Functions</strong>: include identity management, access management, policy management (e.g. to
enforce security policies), etc.</p></li>
<li><p><strong>Compute Resources Manager</strong>: provides a mechanism to provision virtual resources with the help of hardware compute
resources</p></li>
<li><p><strong>Storage Resources Manager</strong>: provides a mechanism to provision virtual resources with the help of hardware storage
resources</p></li>
<li><p><strong>Network Resources Manager</strong>: provides a mechanism to provision virtual resources with the help of hardware network
resources</p></li>
</ul>
</section>
</section>
<section id="hardware-infrastructure-layer">
<h2>Hardware Infrastructure Layer<a class="headerlink" href="#hardware-infrastructure-layer" title="Link to this heading">¶</a></h2>
<section id="hardware-infrastructure-resources">
<h3>Hardware Infrastructure Resources<a class="headerlink" href="#hardware-infrastructure-resources" title="Link to this heading">¶</a></h3>
<p>Compute, Storage and Network resources serve as the foundation of the cloud infrastructure. They are exposed to and used
by a set of networked Host Operating Systems in a cluster that normally handles the Virtual Infrastructure Layer
offering Virtual Machines or Containers where the application workloads (VNFs/CNFs) runs.</p>
<figure class="align-default" id="id52">
<img alt="Cloud Infrastructure Hardware Resources" src="../_images/ch03-model-hardware-resources.png" />
<figcaption>
<p><span class="caption-text">Cloud Infrastructure Hardware Resources</span><a class="headerlink" href="#id52" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>In managed Hardware Infrastructure systems, these consumable Compute, Storage and Network resources can be provisioned
through operator commands or through software APIs. There is a need to distinguish between these consumable resources,
that are treated as leased resources, from the actual physical hardware resources that are installed in the data centre.
For this purpose, the hardware resource layer is conceptually split into a Logical Resource Layer that surfaces the
consumable resources to the software layer above, and the Physical Resource Layer that is operated and managed by the
Cloud Infrastructure Providers Operations team from the Hardware Infrastructure Management functions perspective.</p>
<p>Some installations might use a cluster of managed switches or storage components controlled by a Switch Fabric
controller and/or a Storage Fabric controller acting as an appliance system. These systems should be federated with the
HW Infrastructure Management system over some API to facilitate exchange of configuration intent, status and telemetry
information allowing the Hardware Infrastructure Management and Management stack to automate Cloud Infrastructure
operations. These appliance systems normally also have their own Equipment Management APIs and procedures for the
hardware installation and maintenance staff.</p>
<p>An example could be a Cloud Infrastructure stack federated with a commercial Switch Fabric where the Cloud
Infrastructure shall be able to “send” networking configuration intent to the Switch Fabric and the Switch Fabric shall
be able to “send” (see note below) status and telemetry information to the Cloud Infrastructure e.g. Port/Link Status
and packet counters of many sorts.
This allows Hardware Infrastructure Management and Cloud Infrastructure management stack to have network automation that
includes the switches that are controlled by the federated Switch Fabric. This would be a rather normal case for
Operators that have a separate Networking Department that owns and runs the Switch Fabric separately from the Data
Centre.</p>
<p>NOTE: The word “send” is a very lose definition of getting a message across to the other side, and could be implemented
in many different ways.</p>
<section id="hardware-acceleration-resources">
<h4>Hardware Acceleration Resources<a class="headerlink" href="#hardware-acceleration-resources" title="Link to this heading">¶</a></h4>
<p>For a given software network function and software infrastructure, Hardware Acceleration resources can be used to
achieve requirements or improve cost/performance. Following table gives reasons and examples for using Hardware
Acceleration.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Reason for using Hardware
Acceleration</p></th>
<th class="head"><p>Example</p></th>
<th class="head"><p>Comment</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Achieve technical requirements</p></td>
<td><p>Strict latency or timing accuracy</p></td>
<td><p>Must be done by optimizing compute
node; cannot be solved by adding more
compute nodes</p></td>
</tr>
<tr class="row-odd"><td><p>Achieve technical requirements</p></td>
<td><p>Fit within power or space envelope</p></td>
<td><p>Done by optimizing cluster of compute
nodes</p></td>
</tr>
<tr class="row-even"><td><p>Improve cost/performance</p></td>
<td><p>Better cost and less power/cooling by
improving performance per node</p></td>
<td><p>Used when functionality can be achieved
through usage of accelerator or by
adding more compute nodes</p></td>
</tr>
</tbody>
</table>
<p><strong>Table 3-5:</strong> Reasons and examples for using Hardware Acceleration</p>
<p>Hardware Accelerators can be used to offload software execution for purpose of accelerating tasks to achieve faster
performance, or offloading the tasks to another execution entity to get more predictable execution times, efficient
handling of the tasks or separation of authority regarding who can control the tasks execution.</p>
<p>More details about Hardware Acceleration are in <a class="reference internal" href="#hardware-acceleration-abstraction">hardware acceleration abstraction</a>.</p>
</section>
</section>
<section id="hardware-infrastructure-manager">
<h3>Hardware Infrastructure Manager<a class="headerlink" href="#hardware-infrastructure-manager" title="Link to this heading">¶</a></h3>
<p>The Hardware Infrastructure Manager shall at least support equipment management for all managed physical hardware resources of
the Cloud Infrastructure.</p>
<p>In most deployments the Hardware Infrastructure Manager should also be the HW Infrastructure Layer provisioning manager
of the Compute, Storage and Network resources that can be used by the Virtualization Infrastructure Layer instances. It
shall provide an API enabling vital resource recovery and control functions of the provisioned functions e.g. Reset and
Power control of the Computes.</p>
<p>For deployments with more than one Virtualization Infrastructure Layer instance that will be using a common pool of
hardware resources there is a need for a HW Infrastructure Layer provisioning manager of the Compute, Storage and
Network resources to handle the resource assignment and arbitration.</p>
<p>The resource allocation could be a simple book-keeping of which Virtualization Infrastructure Layer instance that have
been allocated a physical hardware resource or a more advanced resource Composition function that assemble the consumed
Compute, Storage and Network resources on demand from the pools of physical hardware resources.</p>
<figure class="align-default" id="id53">
<img alt="Hardware Infrastructure Manager" src="../_images/ch03-model-hardware-manager.png" />
<figcaption>
<p><span class="caption-text">Hardware Infrastructure Manager</span><a class="headerlink" href="#id53" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>The hardware infrastructure manager allows to:</p>
<ul class="simple">
<li><p>provision, manage, monitor and delete hardware resources</p></li>
<li><p>manage physical hardware resource discovery, monitoring and topology</p></li>
<li><p>manage hardware infrastructure telemetry and log collection services</p></li>
</ul>
<p>The hardware infrastructure manager needs to support the following functional aspects:</p>
<ul class="simple">
<li><p><strong>API/UI</strong>: an application programming interface / user interface providing access to the hardware resource
management functions</p></li>
<li><p><strong>Discovery</strong>: discover physical hardware resources and collect relevant information about them</p></li>
<li><p><strong>Topology</strong>: discover and monitor physical interconnection (e.g. cables) in between the physical hardware resources</p></li>
<li><p><strong>Equipment</strong>: manages the physical hardware resources in terms of configuration, firmware status, health/fault status
and autonomous environmental control functions such as fan and power conversion regulations</p></li>
<li><p><strong>Resource Allocation and Composition</strong>: creates, modifies and deletes logical Compute, Network and Storage Resources
through Composition of allocated physical hardware resources</p></li>
<li><p><strong>Underlay Network Resources Manager</strong>: provides a mechanism to provision hardware resources and provide separation in
between multiple Virtualization Infrastructure instances for the use of the underlay network (e.g. switch fabric,
switches, SmartNICs)</p></li>
<li><p><strong>Monitoring</strong>: monitors and collects information on events, current state and telemetry data of physical hardware
resources, autonomous equipment control functions as well as Switch and Storage Fabric systems</p></li>
<li><p><strong>Additional Management Functions</strong>: include software and configuration life cycle management, identity management,
access management, policy management (e.g. to enforce security policies), etc.</p></li>
</ul>
</section>
</section>
<section id="the-redfish-standard-based-hardware-infrastructure-manager">
<h2>The Redfish® Standard based Hardware Infrastructure Manager<a class="headerlink" href="#the-redfish-standard-based-hardware-infrastructure-manager" title="Link to this heading">¶</a></h2>
<p>This section proposes a Redfish based hardware infrastructure manager for the Anuket project. Open Distributed Infrastructure Management (ODIM) is an open source software platform that delivers means of distributed physical infrastructure lifecycle management based on the industry standard DMTF Redfish API and Model specification.</p>
<p>Chapter 9 (<a class="reference internal" href="chapter09.html#configuration-and-lifecycle-management"><span class="std std-ref">Configuration and Lifecycle Management</span></a>) of this reference model specifies Redfish standard for managing Infrastructure Hardware. The GSMA’s “Cloud Infrastructure Reference Model (NG.126)” also specifies Redfish as the standard interface that should be
made available by the infrastructure and Cloud Infrastructure Management components, in the “Infrastructure Hardware” layer.</p>
<p>Redfish is an internationally recognized standard <a href="#id58"><span class="problematic" id="id59">`ISO/IEC 30115:2018`__</span></a> <span id="id1">[<a class="reference internal" href="chapter01.html#id60" title="Information technology — Redfish scalable platforms management API specification. URL: https://www.iso.org/standard/83853.html.">12</a>]</span>. The Redfish interface specifies a HTTP RESTful interface that a client can use to manage conformant platforms. The Redfish standard consists of a Redfish interface specification <a class="reference external" href="://www.dmtf.org/sites/default/files/standards/documents/DSP0266_1.8.0.pdf">Redfish Specification</a> and model specification <a href="#id58"><span class="problematic" id="id60">`Redfish Data Model Specification`__</span></a> <span id="id2">[<a class="reference internal" href="chapter01.html#id61" title="Redfish Data Model Specification. URL: https://www.dmtf.org/sites/default/files/standards/documents/DSP0268_2022.2.pdf.">13</a>]</span>.  The interface specification defines the RESTful behavior of the resources. The data model specification defines the structure of the HTTP resources. The model is expressed as schema using OpenAPI and json-schema formats. The schema allows implementing redfish clients using the OpenAPI and json-schema toolchains.</p>
<p>There are several opensource implementations of Redfish Clients and Redfish Services.
A refish resource aggregator could implement a scalable infrastructure manager which aggregates and exposes a standards-based Redfish interface and services northbound to an Orchestrator.</p>
<p>DMTF specifies the Redfish interface and model, seeking to expand the manageability domains. The prescription of what subset of the
Redfish model needs to be implemented for a specific manageability domain is left to other standards bodies.  Redfish had defined a
JSON syntax for the prescription call a Redfish Profile and implemented an application which reads the Redfish Profile and tests for
conformance of an implementation <a href="#id58"><span class="problematic" id="id61">`Interop Validato`__</span></a> <span id="id3">[<a class="reference internal" href="chapter01.html#id62" title="Redfish-Interop-Validator. URL: https://github.com/DMTF/Redfish-Interop-Validator.">14</a>]</span>.  The Redfish Profile mechanism is being utilized by the Open Compute Project
and Open Process Automation Forum.</p>
<p>Redfish fulfills the following requirements stated in the above HW Infrastructure Manager section:</p>
<ul class="simple">
<li><p><strong>API/UI</strong>: Redfish exactly specifies a programming interface.  The HTTP interface is accessible remotely via IP and locally via the Ethernet loopback mechanism.</p></li>
<li><p><strong>Discovery</strong>: The Redfish schema provides a physical hardware resource model including relevant information</p></li>
<li><p><strong>Topology</strong>: The Redfish schema contains a cable model</p></li>
<li><p><strong>Equipment</strong>: The Redfish schema contains a physical model and a functional model. The physical model expresses the chassis and containers and interconnects between them and environmental control functions.  The functional model expresses the logical aspects and includes configuration and  firmware status.   Both the physical and functional model have their own health/fault status information</p></li>
<li><p><strong>Resource Allocation and Composition</strong>: The Redfish schema has a composition model through which a client can compose a logical resource by allocating physical resources. <a href="#id58"><span class="problematic" id="id62">`Redfish Composition Whitepaper`__</span></a> <span id="id4">[]</span>.</p></li>
<li><p><strong>Underlay Network Resources Manager</strong>: The Redfish schema has models for fabrics, switches and SmartNICs.</p></li>
<li><p><strong>Monitoring</strong>: The Redfish schema contains event model for the client to receive hardware events and telemetry model for collecting information across the entire model (physical and functional) <a href="#id58"><span class="problematic" id="id63">`Redfish Telemetry Whitepaper`__</span></a> <span id="id5">[]</span></p></li>
<li><p><strong>Additional Management Functions</strong>: The Redfish schema has models for access management and identity management <a href="#id58"><span class="problematic" id="id64">`Redfish Data Model Specification`__</span></a> <span id="id6">[<a class="reference internal" href="chapter01.html#id61" title="Redfish Data Model Specification. URL: https://www.dmtf.org/sites/default/files/standards/documents/DSP0268_2022.2.pdf.">13</a>]</span>.</p></li>
</ul>
</section>
<section id="how-redfish-fits-into-the-etsi-nfv-networking-reference-model">
<h2>How Redfish fits into the ETSI NFV Networking Reference Model<a class="headerlink" href="#how-redfish-fits-into-the-etsi-nfv-networking-reference-model" title="Link to this heading">¶</a></h2>
<p>As shown in the figure below a redfish resource aggregator can play the role of Hardware Infrastructure Manager in the ETSI NFV Networking Reference Model. Whereas this resource manager would expose a Redfish interface to the northbound, the infrastructure pieces can themselves be managed using plugins.</p>
<figure class="align-default" id="odim-fitment-in-the-etsi-nfv-networking-reference-model">
<img alt="ODIM fitment in the ETSI NFV Networking Reference Model" src="../_images/Chapter-3-ODIM-CloudInfraMgmt.png" />
</figure>
<p>The plugins may manage compute, storage and network devices from multiple vendors.</p>
</section>
<section id="network">
<h2>Network<a class="headerlink" href="#network" title="Link to this heading">¶</a></h2>
<p>Networking, alongside Compute and Storage, is an integral part of the Cloud Infrastructure (Network Function
Virtualisation Infrastructure). The general function of networking in this context is to provide the connectivity
between various virtual and physical resources required for the delivery of a network service. Such connectivity may
manifest itself as a virtualised network between VMs and/or containers (e.g. overlay networks managed by SDN
controllers, and/or programmable network fabrics) or as an integration into the infrastructure hardware level for
offloading some of the network service functionality.</p>
<p>Normalization of the integration reference points between different layers of the Cloud Infrastructure architecture is
one of the main concerns. In the networking context the primary focus is directed on the packet flow and control flow
interfaces between the virtual resources (referred to as Software (SW) Virtualisation Layer) and physical resources
(referred to as Hardware (HW) Infrastructure Layer), as well as on related integration into the various MANO reference
points (hardware/network infrastructure management, orchestration). The identification of these two different layers
(SW Virtualisation Layer and HW Infrastructure Layer) remains in alignment with the separation of resources into virtual
and physical resources, generally used in this document, see e.g., <code class="xref std std-numref docutils literal notranslate"><span class="pre">Cloud</span> <span class="pre">Infrastructure</span> <span class="pre">Model</span> <span class="pre">Overview</span></code>. The
importance of understanding the separation of concerns between SW Virtualisation Layer and HW Infrastructure Layer is
important because without it, the cardinality of having multiple CaaS and IaaS instances executing on their own private
virtual resources from the single shared HW Infrastructure Layer cannot be expressed into separate administrative
domains.</p>
<section id="network-principles">
<h3>Network Principles<a class="headerlink" href="#network-principles" title="Link to this heading">¶</a></h3>
<p>Principles that should be followed during the development and definition of the networking scope for the Reference
Model, Reference Architectures, Reference Implementations and Reference Conformance test suites:</p>
<ul class="simple">
<li><p>Abstraction: A standardized network abstraction layer between the Virtualisation Layers and the Network Physical
Resources Layer that hides (or abstracts) the details of the Network Physical resources from the Virtualisation
Layers.</p></li>
</ul>
<blockquote>
<div><p><strong>Note:</strong> In deployment phases this principle may be applied in many different ways e.g. depending on target use case
requirements, workload characteristics, different algorithm implementations of pipeline stages and available
platforms. The network abstraction layer supports, for example, physical resources with or without programmable
hardware acceleration, or programmable network switches</p>
</div></blockquote>
<ul class="simple">
<li><p>Agnosticism: Define Network Fabric concepts and models that can carry any type of traffic in terms of:</p>
<ul>
<li><p>Control, User and Management traffic types</p></li>
<li><p>Acceleration technologies that can support multiple types of infrastructure deployments and network function
workloads</p></li>
</ul>
</li>
<li><p>Automation: Enable end-to-end automation, from Physical Fabric installation and provisioning to automation of
workloads (VNF/CNF) onboarding.</p></li>
<li><p>Openness: All networking is based on open source or standardized APIs (North Bound Interfaces (NBI) and South Bound
Interfaces (SBI)) and should enable integration of open source networking components such as SDN controllers.</p></li>
<li><p>Programmability: Network model enables a programmable forwarding plane controlled from a separately deployed control
plane.</p></li>
<li><p>Scalability: Network model enables scalability to handle all traffic traverse North-South and East-West enabling small
up to large deployments in a non-blocking manner.</p></li>
<li><p>Workload agnostic: Network model is capable of providing connectivity to any type of workloads, including VNF, CNF and
BareMetal workloads.</p></li>
<li><p>Carrier Grade: Network model is capable of supporting deployments of the carrier grade workloads.</p></li>
<li><p>Future proof: Network model is extendible to support known and emerging technology trends including SmartNICs, FPGAs
and Programmable Switches, integrated for multi-clouds, and Edge related technologies.</p></li>
</ul>
</section>
<section id="network-layering-and-concepts">
<h3>Network Layering and Concepts<a class="headerlink" href="#network-layering-and-concepts" title="Link to this heading">¶</a></h3>
<p>The Cloud Infrastructure Networking Reference Model is an essential foundation that governs all Reference Architectures
and Cloud Infrastructure implementations to enable multiple cloud infrastructure virtualisation technology choices and
their evolution. These include:</p>
<ul class="simple">
<li><p>Single Infrastructure as a Service (IaaS) based virtualisation instances with Virtual Machines (VM)</p></li>
<li><p>Multi IaaS based virtualisation instances</p></li>
<li><p>Cloud Native Container as a Service (CaaS) based virtualisation instances, and</p></li>
<li><p>Hybrid multi IaaS and CaaS based virtualisation instances</p></li>
</ul>
<p>To retain the cloud paradigms of automation, scalability and usage of shared hardware resources when introducing CaaS
instances it is necessary to enable an ability to co-deploy multiple simultaneous IaaS and CaaS instances on a shared
pool of hardware resources.</p>
<p>Compute and Storage resources are rarely shared in between IaaS or CaaS instances, but the underpinning networking, most
commonly implemented with Ethernet and IP, must be shared and managed as a shared pool of underlay network resources to
enable the pooled usage of Compute and Storage from a managed shared pool.</p>
<p>Throughout this chapter and its figures a number of references to ETSI NFV are made and they explicitly are made towards
the ETSI NFV models in the Architectural Framework:</p>
<ul class="simple">
<li><p>ETSI GS NFV 002 V1.2.1 cite:p:<cite>etsigsnfv002</cite></p></li>
<li><p>ETSI GR NFV-IFA 029 V3.3.1 <span id="id7">[<a class="reference internal" href="chapter01.html#id14" title="Network Functions Virtualisation (NFV) Release 3; Architecture; Report on the Enhancements of the NFV architecture towards &quot;Cloud-native&quot; and &quot;PaaS&quot;. ETSI GR NFV-IFA 029 v3.3.1. URL: https://www.etsi.org/deliver/etsi_gr/NFV-IFA/001_099/029/03.03.01_60/gr_NFV-IFA029v030301p.pdf.">15</a>]</span></p></li>
</ul>
<p>Cloud and Telco networking are layered, and it is very important to keep the dependencies between the layers low to
enable security, separation and portability in between multiple implementations and generations.</p>
<p>Before we start developing a deep model we need to agree on some foundational concepts and layering that allow
decoupling of implementations in between the layers. We will emphasize four concepts in this section:</p>
<ul class="simple">
<li><p>Underlay and Overlay Networking concepts</p></li>
<li><p>Hardware and Virtual Infrastructure Layer concepts</p></li>
<li><p>Software Defined Underlay and Overlay Networking concepts</p></li>
<li><p>Programmable Networking Fabric concept</p></li>
</ul>
<section id="underlay-and-overlay-networking-concepts">
<h4>Underlay and Overlay Networking Concepts<a class="headerlink" href="#underlay-and-overlay-networking-concepts" title="Link to this heading">¶</a></h4>
<p>The ETSI Network Functions Virtualisation Architectural Framework (as referred above) describes how a Virtual
Infrastructure Layer instance abstracts the hardware resources and separates Virtualisation Tenants (Workload) from each
other. It does also specifically state that the control and implementation of the hardware layer is out of scope for
that specification.</p>
<p>When having multiple Virtual Infrastructure Layer instances on a shared hardware infrastructure, the networking can be
layered in an Underlay and an Overlay Network layer. The purpose with this layering is to ensure separation of the
Virtualisation Tenants (Workload) Overlay Networks from each other, whilst allowing the traffic to flow on the shared
Underlay Network in between all Ethernet connected hardware (HW) devices.</p>
<p>The Overlay Networking separation is often done through encapsulation of Tenants traffic using overlay protocols e.g.
through VxLAN or EVPN on the Underlay Networks e.g. based on L2 (VLAN) or L3 (IP) networks.</p>
<p>The Overlay Network for each Cloud Infrastructure deployment must support a basic primary Tenant Network between the
Instances within each Tenant. Due to the nature of Telecom applications handling of Networks and their related Network
Functions they often need access to external non-translated traffic flows and have multiple separated or secondary
traffic channels with abilities for different traffic treatments.</p>
<p>In some instances, the Virtualisation Tenants can bypass the Overlay Networking encapsulation to achieve better
performance or network visibility/control. A common method to bypass the Overlay Networking encapsulation normally done
by the Virtualisation Layer, is the VNF/CNF usage of SR-IOV that effectively take over the Physical and Virtual
Functions of the NIC directly into the VNF/CNF Tenant. In these cases, the Underlay Networking must handle the
separation e.g. through a Virtual Termination End Point (VTEP) that encapsulate the Overlay Network traffic.</p>
<blockquote>
<div><p><strong>Note:</strong> Bypassing the Overlay Networking layer is a violation of the basic decoupling principles, but is in some
cases unavoidable with existing technologies and available standards. Until suitable technologies and standards are
developed, a set of agreed exemptions has been agreed that forces the Underlay Networking to handle the bypassed
Overlay Networking separation.</p>
</div></blockquote>
<p>VTEP could be manually provisioned in the Underlay Networking or be automated and controlled through a Software Defined
Networking controller interfaces into the underlying networking in the HW Infrastructure Layer.</p>
</section>
<section id="hardware-and-virtual-infrastructure-layer-concepts">
<h4>Hardware and Virtual Infrastructure Layer Concepts<a class="headerlink" href="#hardware-and-virtual-infrastructure-layer-concepts" title="Link to this heading">¶</a></h4>
<p>The Cloud Infrastructure (based on ETSI NFV Infrastructure with hardware extensions) can be considered to be composed of
two distinct layers, here referred to as HW Infrastructure Layer and Virtual Infrastructure Layer. When there are
multiple separated simultaneously deployed Virtual Infrastructure domains, the architecture and deployed implementations
must enable each of them to be in individual non-dependent administrative domains. The HW Infrastructure must then also
be enabled to be a fully separated administrative domain from all of the Virtualisation domains.</p>
<p>For Cloud Infrastructure implementations of multiple well separated simultaneous Virtual Infrastructure Layer instances
on a shared HW Infrastructure there must be a separation of the hardware resources i.e. servers, storage and the
Underlay Networking resources that interconnect the hardware resources e.g. through a switching fabric.</p>
<p>To allow multiple separated simultaneous Virtual Infrastructure Layer instances onto a shared switching fabric there is
a need to split up the Underlay Networking resources into non overlapping addressing domains on suitable protocols e.g.
VxLAN with their VNI Ranges. This separation must be done through an administrative domain that could not be compromised
by any of the individual Virtualisation Infrastructure Layer domains either by malicious or unintentional Underlay
Network mapping or configuration.</p>
<p>These concepts are very similar to how the Hyperscaler Cloud Providers (HCP) offer Virtual Private Clouds for users of
Bare Metal deployment on the HCP shared pool of servers, storage and networking resources.</p>
<p>The separation of Hardware and Virtual Infrastructure Layers administrative domains makes it important that the
Reference Architectures do not include direct management or dependencies of the pooled physical hardware resources in
the HW Infrastructure Layer e.g. servers, switches and underlay networks from within the Virtual Infrastructure Layer.
All automated interaction from the Virtual Infrastructure Layer implementations towards the HW Infrastructure with its
shared networking resources in the HW Infrastructure Layer must go through a common abstracted Reference Model
interface.</p>
</section>
<section id="software-defined-underlay-and-overlay-networking-concepts">
<h4>Software Defined Underlay and Overlay Networking Concepts<a class="headerlink" href="#software-defined-underlay-and-overlay-networking-concepts" title="Link to this heading">¶</a></h4>
<p>A major point with a Cloud Infrastructures is to automate as much as possible. An important tool for Networking
automation is Software Defined Networking (SDN) that comes in many different shapes and can act on multiple layers of
the networking. In this section we will deal with the internal networking of a datacentre and not how datacentres
interconnect with each other or get access to the world outside of a datacentre.</p>
<p>When there are multiple simultaneously deployed instances of the Virtual Infrastructure Layers on the same HW
Infrastructure, there is a need to ensure Underlay networking separation in the HW Infrastructure Layer. This separation
can be done manually through provisioning of a statically configured separation of the Underlay Networking in the HW
Infrastructure Layer. A better and more agile usage of the HW Infrastructure is to offer each instance of the Virtual
Infrastructure Layer a unique instance of a SDN interface into the shared HW Infrastructure. Since these SDN instances
only deal with a well separated portion (or slice) of the Underlay Networking we call this interface SDN-Underlay
(SDNu).</p>
<p>The HW Infrastructure Layer is responsible for keeping the different Virtual Infrastructure Layer instances separated in
the Underlay Networking. This can be done through manual provisioning methods or be automated through a HW
Infrastructure Layer orchestration interface. The separation responsibility is also valid between all instances of the
SDNu interface since each Virtual Infrastructure Layer instance shall not know about, be disturbed by or have any
capability to reach the other Virtual Infrastructure instances.</p>
<p>An SDN-Overlay control interface (here denoted SDNo) is responsible for managing the Virtual Infrastructure Layer
virtual switching and/or routing as well as its encapsulation and its mapping onto the Underlay Networks.</p>
<p>In cases where the VNF/CNF bypasses the Virtual Infrastructure Layer virtual switching and its encapsulation, as
described above, the HW Infrastructure Layer must perform the encapsulation and mapping onto the Underlay Networking to
ensure the Underlay Networking separation. This should be a prioritized capability in the SDNu control interface since
Anuket currently allow exemptions for bypassing the virtual switching (e.g. through SR-IOV).</p>
<p>SDNo controllers can request Underlay Networking encapsulation and mapping to be done by signalling to an SDNu
controller. There are however today no standardized way for this signalling and because of that there is a missing
reference point and API description in this architecture.</p>
<p>Multiple instances of Container as a Service (CaaS) Virtual Infrastructure Layers running on an Infrastructure as a
Service (IaaS) Virtual Infrastructure Layer could make use of the IaaS layer to handle the required Underlay Networking
separation. In these cases, the IaaS Virtualisation Infrastructure Manager (VIM) could include an SDNu control interface
enabling automation.</p>
<blockquote>
<div><p><strong>Note:</strong> The Reference Model describes a logical separation of SDNu and SDNo interfaces to clarify the separation of
administrative domains where applicable. In real deployment cases an Operator can select to deploy a single SDN
controller instance that implements all needed administrative domain separations or have separate SDN controllers for
each administrative domain. A common deployment scenario today is to use a single SDN controller handling both
Underlay and Overlay Networking which works well in the implementations where there is only one administrative domain
that owns both the HW Infrastructure and the single Virtual Infrastructure instance. However a shared Underlay
Network that shall ensure separation must be under the control of the shared HW Infrastructure Layer.
One consequence of this is that the Reference Architectures must not model collapsed SDNo and SDNu controllers since
each SDNo must stay unaware of other deployed implementations in the Virtual Infrastructure Layer running on the same
HW Infrastructure.</p>
</div></blockquote>
</section>
<section id="programmable-networking-fabric-concept">
<h4>Programmable Networking Fabric Concept<a class="headerlink" href="#programmable-networking-fabric-concept" title="Link to this heading">¶</a></h4>
<p>The concept of a Programmable Networking Fabric pertains to the ability to have an effective forwarding pipeline (a.k.a.
forwarding plane) that can be programmed and/or configured without any risk of disruption to the shared Underlay
Networking that is involved with the reprogramming for the specific efficiency increase.</p>
<p>The forwarding plane is distributed by nature and must be possible to implement both in switch elements and on SmartNICs
(managed outside the reach of host software), that both can be managed from a logically centralised control plane,
residing in the HW Infrastructure Layer.</p>
<p>The logically centralised control plane is the foundation for the authoritative separation between different
Virtualisation instances or Bare Metal Network Function applications that are regarded as untrusted both from the shared
layers and each other.</p>
<p>Although the control plane is logically centralized, scaling and control latency concerns must allow the actual
implementation of the control plane to be distributed when required.</p>
<p>All VNF, CNF and Virtualisation instance acceleration as well as all specific support functionality that is programmable
in the forwarding plane must be confined to the well separated sections or stages of any shared Underlay Networking. A
practical example could be a Virtualisation instance or VNF/CNF that controls a NIC/SmartNIC where the Underlay
Networking (Switch Fabric) ensures the separation in the same way as it is done for SR-IOV cases today.</p>
<p>The nature of a shared Underlay Network that shall ensure separation and be robust is that all code in the forwarding
plane and in the control plane must be under the scrutiny and life cycle management of the HW Infrastructure Layer.</p>
<p>This also implies that programmable forwarding functions in a Programmable Networking Fabric are shared resources and by
that will have to get standardised interfaces over time to be useful for multiple VNF/CNF and multi-vendor architectures
such as ETSI NFV. Example of such future extensions of shared functionality implemented by a Programmable Networking
Fabric could be L3 as a Service, Firewall as a Service and Load Balancing as a Service.</p>
<blockquote>
<div><p><strong>Note:</strong> Appliance-like applications that fully own its infrastructure layers (share nothing) could manage and
utilize a Programmable Networking Fabric in many ways, but that is not a Cloud Infrastructure implementation and
falls outside the use cases for these specifications.</p>
</div></blockquote>
</section>
</section>
<section id="networking-reference-model">
<h3>Networking Reference Model<a class="headerlink" href="#networking-reference-model" title="Link to this heading">¶</a></h3>
<p>The Cloud Infrastructure Networking Reference Model depicted in
<code class="xref std std-numref docutils literal notranslate"><span class="pre">Networking</span> <span class="pre">Reference</span> <span class="pre">Model</span> <span class="pre">based</span> <span class="pre">on</span> <span class="pre">the</span> <span class="pre">ETSI</span> <span class="pre">NFV</span></code> is based on the ETSI NFV model
enhanced with Container Virtualisation support and a strict separation of the HW Infrastructure and Virtualization
Infrastructure Layers in NFVI. It includes all above concepts and enables multiple well separated simultaneous
Virtualisation instances and domains allowing a mix of IaaS, CaaS on IaaS and CaaS on Bare Metal on top of a shared HW
Infrastructure.</p>
<p>It is up to any deployment of the Cloud Infrastructure to decide what Networking related objects to use, but all
Reference Architectures have to be able to map into this model.</p>
<figure class="align-default" id="networking-reference-model-based-on-the-etsi-nfv">
<img alt="Networking Reference Model based on the ETSI NFV" src="../_images/RM-Ch03_5-Networking-Reference-Model-based-on-the-ETSI-NFV.png" />
<figcaption>
<p><span class="caption-text">Networking Reference Model based on the ETSI NFV</span><a class="headerlink" href="#networking-reference-model-based-on-the-etsi-nfv" title="Link to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="deployment-examples-based-on-the-networking-reference-model">
<h3>Deployment Examples Based on the Networking Reference Model<a class="headerlink" href="#deployment-examples-based-on-the-networking-reference-model" title="Link to this heading">¶</a></h3>
<section id="switch-fabric-and-smartnic-examples-for-underlay-networking-separation">
<h4>Switch Fabric and SmartNIC Examples For Underlay Networking Separation<a class="headerlink" href="#switch-fabric-and-smartnic-examples-for-underlay-networking-separation" title="Link to this heading">¶</a></h4>
<p>The Hardware Infrastructure Layer can implement the Underlay Networking separation in any type of packet handling component.
This may be deployed in many different ways depending on target use case requirements, workload characteristics and
available platforms. Two of the most common ways are: (1) within the physical Switch Fabric and (2) in a SmartNIC
connected to the Server CPU being controlled over a management channel that is not reachable from the Server CPU and its
host software. In either way the Underlay Networking separation is controlled by the HW Infrastructure Manager.</p>
<p>In both cases the Underlay Networking can be externally controlled over the SDNu interface that must be instantiated
with appropriate Underlay Networking separation for each of the Virtualization administrative domains.</p>
<blockquote>
<div><p><strong>Note:</strong> The use of SmartNIC in this section is only pertaining to Underlay Networking separation of Virtual
instances in separate Overlay domains in much the same way as AWS do with their Nitro SmartNIC. This is the important
consideration for the Reference Model that enables multiple implementation instances from one or several Reference
Architectures to be used on a shared Underlay Network. The use of SmartNIC components from any specific Virtual
instance e.g. for internal virtual switching control and acceleration must be regulated by each Reference
Architecture without interfering with the authoritative Underlay separation laid out in the Reference Model.</p>
</div></blockquote>
<p>Two exemplifications of different common HW realisations of Underlay Network separation in the HW Infrastructure Layer
can be seen in <code class="xref std std-numref docutils literal notranslate"><span class="pre">Underlay</span> <span class="pre">Networking</span> <span class="pre">separation</span> <span class="pre">examples</span></code> below.</p>
<figure class="align-default" id="underlay-networking-separation-examples">
<img alt="Underlay Networking separation examples" src="../_images/RM-Ch03_5-Underlay-Networking-separation-examples.png" />
<figcaption>
<p><span class="caption-text">Underlay Networking separation examples</span><a class="headerlink" href="#underlay-networking-separation-examples" title="Link to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="sdn-overlay-and-sdn-underlay-layering-and-relationship-example">
<h4>SDN Overlay and SDN Underlay layering and relationship example<a class="headerlink" href="#sdn-overlay-and-sdn-underlay-layering-and-relationship-example" title="Link to this heading">¶</a></h4>
<p>Two use case examples with both SDNo and SDNu control functions depicting a software based virtual switch instance in
the Virtual Infrastructure Layer and another high performance oriented Virtual Infrastructure instance (e.g. enabling
SR-IOV) are described in <code class="xref std std-numref docutils literal notranslate"><span class="pre">SDN</span> <span class="pre">Controller</span> <span class="pre">relationship</span> <span class="pre">examples</span></code> (below). The examples are showing how the
encapsulation and mapping could be done in the virtual switch or in a SmartNIC on top of a statically provisioned
underlay switching fabric, but another example could also have been depicted with the SDNu controlling the underlay
switching fabric without usage of SmartNICs.</p>
<figure class="align-default" id="sdn-controller-relationship-examples">
<img alt="SDN Controller relationship examples" src="../_images/RM-Ch03_5-SDN-Controller-relationship-examples.png" />
<figcaption>
<p><span class="caption-text">SDN Controller relationship examples</span><a class="headerlink" href="#sdn-controller-relationship-examples" title="Link to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="example-of-iaas-and-caas-virtualization-infrastructure-instances-on-a-shared-hw-infrastructure-with-sdn">
<h4>Example of IaaS and CaaS Virtualization Infrastructure Instances on a Shared HW Infrastructure With SDN<a class="headerlink" href="#example-of-iaas-and-caas-virtualization-infrastructure-instances-on-a-shared-hw-infrastructure-with-sdn" title="Link to this heading">¶</a></h4>
<p>A Networking Reference Model deployment example is depicted in <code class="xref std std-numref docutils literal notranslate"><span class="pre">Networking</span> <span class="pre">Reference</span> <span class="pre">Model</span> <span class="pre">deployment</span> <span class="pre">example</span></code>
(below) to demonstrate the mapping to ETSI NFV reference points with additions of packet flows through the
infrastructure layers and some other needed reference points. The example illustrates individual responsibilities of a
complex organization with multiple separated administrative domains represented with separate colours.</p>
<p>The example is or will be a common scenario for operators that modernise their network functions during a rather long
period of migration from VNFs to Cloud Native CNFs. Today the network functions are predominantly VNFs on IaaS
environments and the operators are gradually moving a selection of these into CNFs on CaaS that either sit on top of the
existing IaaS or directly on Bare Metal. It is expected that there will be multiple CaaS instances in most networks,
since it is not foreseen any generic standard of a CaaS implementation that will be capable to support all types of CNFs
from any vendor. It is also expected that many CNFs will have dependencies to a particular CaaS version or instances
which then will prohibit a separation of Life Cycle Management in between individual CNFs and CaaS instances.</p>
<figure class="align-default" id="networking-reference-model-deployment-example">
<img alt="Networking Reference Model deployment example" src="../_images/RM-Ch03_5-Networking-Reference-Model-deployment-example.png" />
<figcaption>
<p><span class="caption-text">Networking Reference Model deployment example</span><a class="headerlink" href="#networking-reference-model-deployment-example" title="Link to this image">¶</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="service-function-chaining">
<h3>Service Function Chaining<a class="headerlink" href="#service-function-chaining" title="Link to this heading">¶</a></h3>
<p>Over the past few years there has been a significant move towards decomposing network functions into smaller
sub-functions that can be independently scaled and potentially reused across multiple network functions. A service chain
allows composition of network functions by passing selected packets through multiple smaller services.</p>
<p>In order to support this capability in a sustainable manner, there is a need to have the capability to model service
chains as a high level abstraction. This is essential to ensure that the underlying connection setup, and (re-)direction
of traffic flows can be performed in an automated manner. At a very high level a service chain can be considered a
directed acyclic graph with the composing network functions being the vertices. Building on top of this, a service chain
can be modelled by defining two parameters:</p>
<ul class="simple">
<li><p>An acyclic graph defining the service functions that need to be traversed for the service chain. This allows for
multiple paths for a packet to traverse the service chain.</p></li>
<li><p>A set of packet/flow classifiers that determine what packets will enter and exit a given service chain</p></li>
</ul>
<p>These capabilities need to be provided for both virtualised and containerised (cloud-native) network functions as there
will be a need to support both of them for the foreseeable future. Since virtualised network functions have existed for
a while there is existing, albeit partial, support for service chaining in virtualised environments in orchestration
platforms like OpenStack. Container orchestration platforms such as Kubernetes don’t support service chaining and may
require development of new primitives in order to support advanced networking functions.</p>
<p>It is expected that reference architectures will provide a service chain workflow manager that would accept the service
function acyclic graph and be able to identify/create the necessary service functions and the networking between them in
order to instantiate such a chain.</p>
<p>There is also a need to provide specialised tools to aid troubleshooting of individual services and the communication
between them in order to investigate issues in the performance of composed network functions. Minimally, there is a need
to provide packet level and byte level counters and statistics as the packets pass through the service chain in order to
ascertain any issues with forwarding and performance. Additionally, there is a need for mechanisms to trace the paths of
selected subsets of traffic as they flow through the service chain.</p>
<section id="service-function-chaining-model-introduction">
<h4>Service Function Chaining Model Introduction<a class="headerlink" href="#service-function-chaining-model-introduction" title="Link to this heading">¶</a></h4>
<p>Service Function Chaining (SFC) can be visualized as a layered structure where the Service Function plane (SFC data
plane, consists of service function forwarder, classifier, service function, service function proxy) resides over a
Service Function overlay network.
SFC utilizes a service-specific overlay that creates the service topology. The service overlay provides service function
connectivity built “on top” of the existing network topology. It leverages various overlay network technologies (e.g.,
Virtual eXtensible Local Area Network (VXLAN)) for interconnecting SFC data-plane elements and allows establishing
Service Function Paths (SFPs).</p>
<p>In a typical overlay network, packets are routed based on networking principles and use a suitable path for the packet
to be routed from a source to its destination.</p>
<p>However, in a service-specific overlay network, packets are routed based on policies. This requires specific support at
network level such as at CNI in CNF environment to provide such specific routing mechanism.</p>
</section>
<section id="sfc-architecture">
<h4>SFC Architecture<a class="headerlink" href="#sfc-architecture" title="Link to this heading">¶</a></h4>
<p>The SFC Architecture is composed of functional management, control and data components as categorised in the Table 3-6
below.</p>
<p>The table below highlights areas under which common SFC functional components can be categorized.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Components</p></th>
<th class="head"><p>Example</p></th>
<th class="head"><p>Responsibilities</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td rowspan="4"><p>Management</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">SFC</span> <span class="pre">orchestrator</span></code></p></td>
<td><p>High Level of orchestrator Orchestrate the SFC based on SFC Models/Policies
with help of control components.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SFC</span> <span class="pre">OAM</span> <span class="pre">Components</span></code></p></td>
<td><p>Responsible for SFC OAM functions</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">VNF</span> <span class="pre">MANO</span></code></p></td>
<td><p>NFVO, VNFM, and VIM Responsible for SFC Data components lifecycle</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">CNF</span> <span class="pre">MANO</span></code></p></td>
<td><p>CNF DevOps Components Responsible for SFC data components lifecycle</p></td>
</tr>
<tr class="row-even"><td rowspan="2"><p>Control</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">SFC</span> <span class="pre">SDN</span> <span class="pre">Controller</span></code></p></td>
<td><p>SDNC responsible to create the service specific overlay network. Deploy
different techniques to stitch the wiring but provide the same functionality,
for example l2xconn, SRv6 , Segment routing etc.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SFC</span> <span class="pre">Renderer</span></code></p></td>
<td><p>Creates and wires ports/interfaces for SF data path</p></td>
</tr>
<tr class="row-even"><td><p>Data</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Core</span> <span class="pre">Components</span></code>SF, SFF, SF Proxy</p></td>
<td><p>Responsible for steering the traffic for intended service functionalities
based on Policies</p></td>
</tr>
</tbody>
</table>
<p><strong>Table 3-6:</strong> SFC Architecture Components</p>
<blockquote>
<div><p><strong>Note:</strong> These are logical components and listed for their functionalities only.</p>
</div></blockquote>
<p>The SFC Architecture components can be viewed as:-</p>
<p><code class="xref std std-numref docutils literal notranslate"><span class="pre">SFC</span> <span class="pre">Architecture</span> <span class="pre">for</span> <span class="pre">VNF</span> <span class="pre">based</span> <span class="pre">SFs</span></code> shows a simple architecture of an SFC with multiple VNFs, as SF data
plane components, along with SFC management and NFV MANO components.</p>
<figure class="align-default" id="sfc-architecture-for-vnf-based-sfs">
<img alt="SFC Architecture for VNF based SFs" src="../_images/ch03-model-sfc-architecture-vnf-2.png" />
<figcaption>
<p><span class="caption-text">SFC Architecture for VNF based SFs</span><a class="headerlink" href="#sfc-architecture-for-vnf-based-sfs" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p><code class="xref std std-numref docutils literal notranslate"><span class="pre">SFC</span> <span class="pre">Architecture</span> <span class="pre">for</span> <span class="pre">CNF</span> <span class="pre">based</span> <span class="pre">SFs</span></code> shows a simple architecture of an SFC with multiple CNFs, as SF data
plane components, along with SFC management and CNF MANO components.</p>
<figure class="align-default" id="sfc-architecture-for-cnf-based-sfs">
<img alt="SFC Architecture for CNF based SFs" src="../_images/ch03-model-sfc-architecture-cnf-2.png" />
<figcaption>
<p><span class="caption-text">SFC Architecture for CNF based SFs</span><a class="headerlink" href="#sfc-architecture-for-cnf-based-sfs" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>The SFC management components together with the control components are responsible for rendering SFC requests to Service
Function paths. For this they convert requisite SFC policies into network topology dependent paths and forwarding
steering policies. Relevant SFC data components - classifiers, service function forwarders - are responsible for
managing the steering policies.</p>
</section>
<section id="information-flows-in-service-function-chaining">
<h4>Information Flows in Service Function Chaining<a class="headerlink" href="#information-flows-in-service-function-chaining" title="Link to this heading">¶</a></h4>
<section id="creation-of-service-function-chain">
<h5>Creation of Service Function Chain<a class="headerlink" href="#creation-of-service-function-chain" title="Link to this heading">¶</a></h5>
<p>The creation of the SFC might include design/preparation phase as:</p>
<ul class="simple">
<li><p>The service functions that are included in the SFC.</p></li>
<li><p>The routing order in the service function, if the SFC is composed of more than one service function.</p></li>
</ul>
<p><code class="xref std std-numref docutils literal notranslate"><span class="pre">Creation</span> <span class="pre">of</span> <span class="pre">Service</span> <span class="pre">Function</span> <span class="pre">Chain</span></code> shows SFC creation call flow, separated logically in two steps.</p>
<figure class="align-default" id="id8">
<img alt="Creation of Service Function Chain" src="../_images/ch03-model-sfc-info-create-flow.png" />
<figcaption>
<p><span class="caption-text">Creation of Service Function Chain</span><a class="headerlink" href="#id8" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<ol class="arabic simple">
<li><p>Creation of service functions of SFC.</p></li>
</ol>
<ul>
<li><p>The flow of steps to enable the SFC creation can be as follows:</p>
<ol class="loweralpha">
<li><p>SFC orchestrator creates the SFs with help of VNF MANO or CNF MANO.</p></li>
<li><p>SFC Renderer attaches the SFC aware interfaces at SFs to enable Service plane</p></li>
<li><p>NFVO boots up the relevant SF configurations at SF.</p>
<p><strong>Note:</strong> These steps are optional, if SFC orchestrator discovers that SFs are already created and existing.</p>
</li>
</ol>
</li>
</ul>
<ol class="arabic simple" start="2">
<li><p>Creation of Service Function Path (SFP) using the created SFs and associated interfaces.</p></li>
</ol>
<ul>
<li><p>A Service Function Path consists of:</p>
<blockquote>
<div><ul class="simple">
<li><p>A set of ports( in VNF environment) or interfaces ( in CNF environment) , that define the sequence of service
functions</p></li>
<li><p>A set of flow classifiers that specify the classified traffic flows entering the chain.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>This step creates a new chain policy with chain rules. Chain rules can include the identifier of a traffic flow,
service characteristics, the SFC identifier and related information to route the packets along the chain. Service
characteristics can be application layer matching information (e.g., URL). Traffic flow identifier can be kind of
traffic (e.g., Video, TCP, HTTP) flow need to be serviced. It can be specific Subscriber to apply service (e.g.,
parental control). The SFC identifier to steer the matched traffic along the SFP with SFC encapsulation.</p>
<ol class="loweralpha simple">
<li><p>SFC orchestrator creates SFP with help of SDNC.</p></li>
<li><p>SDNC pushes the SFC traffic steering policies to SFF(s).</p></li>
<li><p>SFC classifier Policy provided for SFP to SFC classifier by SFC Controller. <strong>Note:</strong> not shown in call flow.</p></li>
</ol>
</li>
</ul>
</section>
<section id="updating-service-function-chain">
<h5>Updating Service Function Chain<a class="headerlink" href="#updating-service-function-chain" title="Link to this heading">¶</a></h5>
<p>SFP or SFC can be updated for various reasons and some of them are:</p>
<ul class="simple">
<li><p>SFC controller monitors the SFP status and alerts SFC controller in case of not meeting SLA or some anomaly.</p></li>
<li><p>SFC design changes to update SF order, inclusion/removal of SFs</p></li>
<li><p>SFC Policy Rules changes</p></li>
</ul>
</section>
<section id="data-steering-in-service-function-chain">
<h5>Data Steering in Service Function Chain<a class="headerlink" href="#data-steering-in-service-function-chain" title="Link to this heading">¶</a></h5>
<p><code class="xref std std-numref docutils literal notranslate"><span class="pre">Data</span> <span class="pre">steering</span> <span class="pre">in</span> <span class="pre">Service</span> <span class="pre">Function</span> <span class="pre">Chain</span></code> shows traffic steering along SFP.</p>
<figure class="align-default" id="id9">
<img alt="Data steering in Service Function Chain" src="../_images/ch03-model-sfc-data-flow.png" />
<figcaption>
<p><span class="caption-text">Data steering in Service Function Chain</span><a class="headerlink" href="#id9" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>SFC classifier detects the traffic flow based on classification policies. For example, to enable SGi-Lan feature as
SFC, 5G User plane function (UPF) acts as SFC classifier. UPF receives the classification policies from 5G Policy
control function (PCF) as traffic steering policies.</p></li>
<li><p>SFC classifier applies the SFC encapsulation (e.g., SCH, NSH) and routes traffic towards SFF, acts as entry point to
SFP. The SFC Encapsulation provides, at a minimum, SFP identification, and is used by the SFC-aware functions, such as
the SFF and SFC-aware SFs.</p></li>
<li><p>SFF based on SFC encapsulation routes the traffic to SF for service functionalities.</p></li>
<li><p>SF updates the SFC encapsulation based on its policies for further services.</p></li>
<li><p>At end of SFP, SFC encapsulation is removed and packet is routed out of SFP.</p></li>
</ul>
</section>
</section>
</section>
<section id="time-sensitive-networking">
<h3>Time Sensitive Networking<a class="headerlink" href="#time-sensitive-networking" title="Link to this heading">¶</a></h3>
<p>Many network functions have time sensitivity for processing and require high precision synchronized clock for the Cloud
Infrastructure. Subset of these workloads, like RAN, in addition require support for Synchronous Ethernet as well.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Reason for using Synchronous
Precision Clock</p></th>
<th class="head"><p>Example</p></th>
<th class="head"><p>Comment</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Achieve technical requirements</p></td>
<td><p>Strict latency or timing accuracy</p></td>
<td><p>Must be done for precise low latency
communication between data source and receiver</p></td>
</tr>
<tr class="row-odd"><td><p>Achieve technical requirements</p></td>
<td><p>Separation of processing pipeline</p></td>
<td><p>Ability to separate RAN into RU, DU, CU on
different or stretch clusters</p></td>
</tr>
</tbody>
</table>
<p><strong>Table 3-7:</strong> Reasons and examples for Precise Clock and Synchronization</p>
<p>Precise Synchronization require specialized card that can be on server or network device motherboard or be part of NIC
or both.</p>
<p>OpenStack and Kubernetes clusters use Network Time Protocol (NTP)
(Protocol and Algorithms Specification <span id="id10">[<a class="reference internal" href="chapter01.html#id7" title="Jim Martin, Jack Burbank, William Kasch, and Professor David L. Mills. Network Time Protocol Version 4: Protocol and Algorithms Specification. RFC 5905, June 2010. URL: https://www.rfc-editor.org/info/rfc5905.">16</a>]</span>,
Autokey Specification <span id="id11">[<a class="reference internal" href="chapter01.html#id8" title="Professor David L. Mills and Brian Haberman. Network Time Protocol Version 4: Autokey Specification. RFC 5906, June 2010. URL: https://www.rfc-editor.org/info/rfc5906.">17</a>]</span>,
Managed Objects <span id="id12">[<a class="reference internal" href="chapter01.html#id9" title="Chris Elliott, Brian Haberman, and Heiko Gerstung. Definitions of Managed Objects for Network Time Protocol Version 4 (NTPv4). RFC 5907, June 2010. URL: https://www.rfc-editor.org/info/rfc5907.">18</a>]</span>,
Server Option for DHCPv6 <span id="id13">[<a class="reference internal" href="chapter01.html#id10" title="Benoit Lourdelet and Richard Gayraud. Network Time Protocol (NTP) Server Option for DHCPv6. RFC 5908, June 2010. URL: https://www.rfc-editor.org/info/rfc5908.">19</a>]</span>)
as the default time synchronization for the cluster. That level of synchronization is not sufficient for some
network functions. Just like real-time operating systems instead of base OS, so is precision timing for clock
synchronization. Precision Time Protocol version 2 <a href="#id58"><span class="problematic" id="id65">`PTP`__</span></a> <span id="id14">[]</span> __ <span id="id15">[<a class="reference internal" href="chapter01.html#id37" title="Precision Clock Synchronization Protocol for Networked Measurement and Control Systems. IEEE 1588-2019. URL: https://standards.ieee.org/standard/1588-2019.html.">20</a>]</span>
is commonly used for Time-Sensitive Networking. This allow synchronization in microsecond range rather than
millisecond range that NTP provides.</p>
<p>Some Network functions, like vDU, of vRAN, also require <a href="#id58"><span class="problematic" id="id66">`SyncE`__</span></a> <span id="id16">[]</span> __ <span id="id17">[<a class="reference internal" href="chapter01.html#id38" title="Timing characteristics of a synchronous equipment slave clock. ITU-T G.8262. URL: https://www.itu.int/rec/T-REC-G.8262.">21</a>]</span>. Control,
User and Synchronization (CUS) Plane specification defines different topology options that provides Lower Layer Split
Control plane 1-4 (LLS-C1 - LLS-C4) with different synchronization requirements
(<a href="#id58"><span class="problematic" id="id67">`ITU-T G.8275.2`__</span></a> <a href="#id18"><span class="problematic" id="id19">:cite:p:`ITU-T G.8275.2`__</span></a> <span id="id20">[<a class="reference internal" href="chapter01.html#id39" title="Precision time protocol telecom profile for time/phase synchronization with partial timing support from the network. ITU-T G.8275.2. URL: https://www.itu.int/rec/T-REC-G.8275.2.">22</a>]</span>).</p>
<p>SyncE was standardized by the ITU-T, in cooperation with IEEE, as three recommendations:</p>
<ul class="simple">
<li><p>ITU-T Rec. G.8261 that defines aspects about the architecture and the wander performance of SyncE networks</p></li>
<li><p>ITU-T Rec. G.8262 that specifies Synchronous Ethernet clocks for SyncE</p></li>
<li><p>ITU-T Rec. G.8264 that describes the specification of Ethernet Synchronization Messaging Channel (ESMC)
SyncE architecture minimally requires replacement of the internal clock of the Ethernet card by a phase locked loop
in order to feed the Ethernet PHY.</p></li>
</ul>
</section>
<section id="load-balancer">
<h3>Load Balancer<a class="headerlink" href="#load-balancer" title="Link to this heading">¶</a></h3>
<p>Load Balancing is the process of distributing a set of tasks over a set of resources (computing units), with the goal
of improving overall system’s scalability and availability. Load balancing can optimize the response time and avoid
unevenly overloading some compute nodes while other compute nodes are left idle.</p>
<p>Two main approaches to Load Balancing are: static algorithms which do not consider the state of different machines,
and dynamic algorithms which are usually more general and more efficient but require exchanges of information between
the different compute units, with possible tradeoff of integration complexity and reduced efficiency.</p>
<p>Load Balancers can be categorized or configured in following ways:</p>
<ul class="simple">
<li><p>based on hardware devices or written in software,</p></li>
<li><p>operate on network Layer 4 or 7,</p></li>
<li><p>balance incoming traffic/requests using target selection like static: random, round robin, based on performance of target, based on client IP address, based on URL path or hash; and dynamic: least connections, least response time.</p></li>
</ul>
<p>To increase redundancy, multiple replicas of load balancer are placed into a load balancing cluster.</p>
</section>
<section id="kubernetes-networking-semantics">
<h3>Kubernetes Networking Semantics<a class="headerlink" href="#kubernetes-networking-semantics" title="Link to this heading">¶</a></h3>
<p>The support for traditional network orchestration is non-existent in Kubernetes as it is foremost a Platform as a
Service (PaaS) environment and not an Infrastructure as a Service (Iaas) component. There is no network orchestration
API, like Neutron in OpenStack, and there is no way to create L2 networks, instantiate network services such as L3aaS
and LBaaS and then connect them all together as can be done using Neutron.</p>
<p>Kubernetes networking can be divided into two parts, built in network functionality available through the pod’s
mandatory primary interface and network functionality available through the pod’s optional secondary interfaces.</p>
<section id="built-in-kubernetes-network-functionality">
<h4>Built in Kubernetes Network Functionality<a class="headerlink" href="#built-in-kubernetes-network-functionality" title="Link to this heading">¶</a></h4>
<p>Kubernetes currently only allows for one network, the <em>cluster</em> network, and one network attachment for each pod. All
pods and containers have an <em>eth0</em> interface, this interface is created by Kubernetes at pod creation and attached to
the cluster network. All communication to and from the pod is done through this interface. Allowing only for one
interface in a pod removes the need for traditional networking tools such as <em>VRFs</em> and additional routes and routing
tables inside the pod network namespace.</p>
</section>
<section id="multiple-networks-and-advanced-configurations">
<h4>Multiple Networks and Advanced Configurations<a class="headerlink" href="#multiple-networks-and-advanced-configurations" title="Link to this heading">¶</a></h4>
<p>Currently Kubernetes does not in itself support multi networks, pod multi network attachments or network orchestration.
This is supported by using a <cite>Container Network Interface</cite> <a href="#id21"><span class="problematic" id="id22">:cite:p:`container-network-interface`__</span></a> multiplexer
such as <a href="#id58"><span class="problematic" id="id68">`Multus`__</span></a> <span id="id23">[]</span>.
The <a href="#id58"><span class="problematic" id="id69">`Network Plumbing Working Group`__</span></a> <span id="id24">[]</span> has produced
the <a href="#id58"><span class="problematic" id="id70">`Kubernetes Network Custom Resource Definition De-facto Standard`__</span></a>
<span id="id25">[]</span>.
This document describes how secondary networks can be defined and attached to
pods.</p>
</section>
</section>
</section>
<section id="storage">
<h2>Storage<a class="headerlink" href="#storage" title="Link to this heading">¶</a></h2>
<section id="introduction-to-storage">
<h3>Introduction to Storage<a class="headerlink" href="#introduction-to-storage" title="Link to this heading">¶</a></h3>
<p>The general function of storage subsystem is to provide the persistent data store required for the delivery of a network
service. In the context of Cloud Infrastructure the storage sub-system needs to accommodate needs of: the tenanted
applications and the platform management.
Each of:</p>
<ul class="simple">
<li><p>underlying compute host boot and virtual machine hosting,</p></li>
<li><p>control plane configuration and management plane storage for fault and performance management and automation,
capacity management and reporting and</p></li>
<li><p>tenant application and VNF storage needs</p></li>
</ul>
<p>have common and specific needs for storage in terms of performance, capacity and consumption models.</p>
<p>The combination of common but diverse needs in conjunction with the differences in the hosting environments (from large
data-centres to small edge deployments) has resulted in the proliferation of storage technologies and their deployment
architectures. To address this the “Reference Model” outlines a “General Cloud Storage Model”
(see <code class="xref std std-numref docutils literal notranslate"><span class="pre">General</span> <span class="pre">Cloud</span> <span class="pre">Storage</span> <span class="pre">Model</span></code> - “General Cloud Storage Model”). The model will outline the different
types of storage technologies and how they can be used to meet the need for:</p>
<ul class="simple">
<li><p>providing storage via dedicated storage systems,</p></li>
<li><p>multi-tenant cloud storage,</p></li>
<li><p>Control and Management Plane storage needs,</p></li>
</ul>
<p>across both large data-centres and small edge deployments; the model can then be used for implementing Reference
Architectures.</p>
<figure class="align-default" id="general-cloud-storage-model">
<img alt="General Cloud Storage Model" src="../_images/rm-chap3.6-general-cloud-storage-model-01.png" />
<figcaption>
<p><span class="caption-text">General Cloud Storage Model</span><a class="headerlink" href="#general-cloud-storage-model" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>Storage is multi-faceted and so can be classified based on its: cost, performance (IOPS, throughput, latency), capacity
and consumption model (platform native, network shared, object or archival) and the underlying implementation model (in
chassis, software defined, appliance). The objective of the model and set of stereotypes and perspectives is to provide
guidance to architects and implementers in establishing storage solutions for Cloud Infrastructure.</p>
<p>The following principles apply to Storage scope for the Reference Model, Reference Architectures, Reference
Implementations and Reference Conformance test suites:</p>
<ul class="simple">
<li><p>Abstraction: A standardized storage abstraction layer between the Virtualisation Layers and the Storage Physical
Resources Layer that hides (or abstracts) the details of the Storage Physical resources from the Virtualisation
Layers.</p></li>
<li><p>Agnosticism: Define Storage subsystem concepts and models that can provide various storage types and performance
requirements (more in section <a class="reference internal" href="#virtual-storage">Virtual Storage</a>).</p></li>
<li><p>Automation: Enable end-to-end automation, from Physical Storage installation and provisioning to automation of
workloads (VNF/CNF) onboarding.</p></li>
<li><p>Openness: All storage is based on open source or standardized APIs (North Bound Interfaces (NBI) and South Bound
Interfaces (SBI)) and should enable integration of storage components such as Software Defined Storage controllers.</p></li>
<li><p>Scalability: Storage model enables scalability to enable small up to large deployments.</p></li>
<li><p>Workload agnostic: Storage model can provide storage functionality to any type of workloads, including: tenant VNF,
CNF and Infrastructure Management whether this is via BareMetal or Virtualised Deployments.</p></li>
<li><p>Operationally Amenable: The storage must be amenable to consistent set of operational processes for: Non-Disruptive
Capacity Expansion and Contraction, Backup/Restoration and Archive and Performance Management. Where applicable
(examples are: Backup/Restoration/Archive) these processes should also be able to be provided to tenants for their own
delegated management.</p></li>
<li><p>Security Policy Amenable: The storage sub-systems must be amenable to policy based security controls covering areas
such as: Encryption for Data at Rest / In Flight, Delegated Tenant Security Policy Management, Platform Management
Security Policy Override, Secure Erase on Device Removal and others</p></li>
<li><p>Future proof: Storage model is extendible to support known and emerging technology trends covering spectrum of
memory-storage technologies including Software Defined Storage with mix of SATA- and NVMe-based SSDs, DRAM and
Persistent Memory, integrated for multi-clouds, and Edge related technologies.</p></li>
</ul>
<p>The above principles should be understood as storage specific specialisations of the
<a class="reference external" href="https://cntt.readthedocs.io/en/latest/common/chapter00.html#anuket-general-principles" title="(in Anuket Specifications)"><span>Anuket General Principles</span></a>.</p>
</section>
<section id="storage-implementation-stereotypes">
<h3>Storage Implementation Stereotypes<a class="headerlink" href="#storage-implementation-stereotypes" title="Link to this heading">¶</a></h3>
<p>The following set of storage implementations outline some of the most prevalent stereotypical storage implementations.</p>
<p>The first of these are for Data Centre Storage cases, with stereotypes of:</p>
<ul class="simple">
<li><p>Dedicated storage appliance (<code class="xref std std-numref docutils literal notranslate"><span class="pre">Storage</span> <span class="pre">Appliance</span> <span class="pre">Stereotype</span></code>) - that provide network based storage via
iSCSI (2), NFS/CIFS (3) with potentially virtual NAS (vNAS) (4) capability. Having virtual network software (4)
allows the establishment of storage tenancies, where storage tenancy have their own virtual storage services which
are exposed on their own network,</p></li>
<li><p>Software defined storage (<code class="xref std std-numref docutils literal notranslate"><span class="pre">Software</span> <span class="pre">Defined</span> <span class="pre">Storage</span> <span class="pre">Stereotype</span></code>) - which is able to provide similar
capabilities as the dedicated storage appliance (see (3),(4) &amp; (5) in diagram). In this case this is provided as a
software solution on top of a hyper-converged infrastructure.</p></li>
</ul>
<figure class="align-default" id="storage-appliance-stereotype">
<img alt="Storage Appliance Stereotype" src="../_images/rm-chap3.6-general-cloud-storage-appliance-sterotype-01.png" />
<figcaption>
<p><span class="caption-text">Storage Appliance Stereotype</span><a class="headerlink" href="#storage-appliance-stereotype" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="software-defined-storage-stereotype">
<img alt="Software Defined Storage Stereotype" src="../_images/rm-chap3.6-general-cloud-storage-software-defined-sterotype-01.png" />
<figcaption>
<p><span class="caption-text">Software Defined Storage Stereotype</span><a class="headerlink" href="#software-defined-storage-stereotype" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>Both of these stereotypes can be used to support very broad storage needs from: machine boot (via iSCSI), providing
storage to the Cloud Platform Control and Management Planes, Platform Native (viz., Hypervisor Attached and Container
Persistence storage, as defined in section <a class="reference internal" href="#storage-for-tenant-consumption"><span class="std std-ref">Storage for Tenant Consumption</span></a>”) and
Application/VNF/CNF managed network storage. To provide this requires connectivity within the Cloud Infrastructure
Underlay and Tenant Overlay networks.</p>
<p>Successful management of Cloud Infrastructure requires high levels of automation, including the ability to stand
up rapidly new storage and hosting infrastructure. This Cloud Infrastructure boot-strapping process is managed through
Infrastructure Automation tooling. A typical part of the boot-strap process is to use PXE (Pre-boot Execution Environment) boot to manage the deployment
of initial images to physical hosts and a similar approach is used for “Bare Metal-as-a-Service” provisioning. The
storage stereotype that covers this use case is:</p>
<ul class="simple">
<li><p>Infrastructure Automation (<code class="xref std std-numref docutils literal notranslate"><span class="pre">Infrastructure</span> <span class="pre">Automation</span> <span class="pre">-</span> <span class="pre">PXE</span> <span class="pre">Boot</span> <span class="pre">Server</span> <span class="pre">Stereotype</span></code>) - where PXE Boot Server
provides a cache of boot images that are stored in local storage (2) which are then conditionally served up as PXE
boot images (3). The PXE boot server can run within bootstrap management hosting in data-centre or within the
routing / switch layer for an edge deployment case aimed to minimise physical footprint. The Infrastructure
Automation PXE server is aware of the provisioning status of the physical infrastructure and will serve specific
images or even not respond to PXE boot requests for hosts which have already been provisioned and are considered
“in service”.</p></li>
</ul>
<figure class="align-default" id="infrastructure-automation-pxe-boot-server-stereotype">
<img alt="Infrastructure Automation - PXE Boot Server Stereotype" src="../_images/rm-chap3.6-general-cloud-storage-infrastructure-automation-pxe-server-sterotype-01.png" />
<figcaption>
<p><span class="caption-text">Infrastructure Automation - PXE Boot Server Stereotype</span><a class="headerlink" href="#infrastructure-automation-pxe-boot-server-stereotype" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>To provide PXE boot service to the underlying resource hosts, the PXE server must be connected to the same network as
the NIC that is configured for PXE boot. The “Infrastructure Automation - PXE Server” stereotype is also applicable to
booting tenant Virtual Machines. In this case, the PXE server is on the same network as one of the machines vNICs. For
tenant use this is provided as part of tenant consumable boot infrastructure services.</p>
<p>For each of the defined stereotypes, the storage service uses physical Block storage for boot (Physical Layer - Block
Consumption -&gt; OS File Systems Exposure (1) on stereotype diagrams). This is the primary use case for use of in chassis
physical storage, that is not being used for consumption and exposure as network-based storage. In general, it is
desirable to use network based storage solution for provision of Cloud Infrastructure storage. The “Infrastructure
Automation - PXE Server” is an exception to the preferential use of network-based storage, and as it is managing the
bootstrap process, it cannot be dependent on a separate storage system for maintaining its image cache.</p>
</section>
<section id="storage-for-tenant-consumption">
<h3>Storage for Tenant Consumption<a class="headerlink" href="#storage-for-tenant-consumption" title="Link to this heading">¶</a></h3>
<p>Storage is made available for tenant consumption through a number of models. A simplified view of this is provided in the
following illustrative model.</p>
<figure class="align-default" id="id54">
<img alt="Storage Model - Cost vs Performance with Consumption Model Overlay" src="../_images/rm-ch3.6-storage-model-02.png" />
<figcaption>
<p><span class="caption-text">Storage Model - Cost vs Performance with Consumption Model Overlay</span><a class="headerlink" href="#id54" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>Where:</p>
<ul class="simple">
<li><p>(Comparative) Cost - is monetary value / unit of end user storage capacity</p></li>
<li><p>Performance - is defined by IOPS / Latency / Throughput as typically each of these increases with successive
generations of storage</p></li>
<li><p>Capacity - consumption needs are represented by width of the: Ultra High Performance, Enterprise Transactional, Value
and Capacity storage options.</p></li>
<li><p>Storage Types - is how the storage is accessed and used, where:</p>
<ul>
<li><p>Platform Native - is managed by the hypervisor / platform (examples are a virtual disk volume from which a VNF
boots and can write back to, the storage interface that is exposed by the container runtime), this storage is
typically not shared across running VNF / CNF instances;</p></li>
<li><p>Shared File Storage - is storage that is accessed through a file systems interface (examples are network based
storage such as CIFS or NFS) where the storage volumes can be accessed and shared by multiple VNF / CNF instances;</p></li>
<li><p>Object Storage - is storage that is accessed via API interfaces (the most common example being HTTP restful
services API), which support get/put of structured objects; and</p></li>
<li><p>Archival - is storage that is targeted for provision of long term storage for purpose of disaster recovery, meeting
legal requirements or other historical recording where the storage mechanism may go through multiple stages before
landing at rest.</p></li>
</ul>
</li>
</ul>
<p>The storage model provides a relatively simple way for the storage consumer to specify / select their storage needs.
This is shown in the following table which highlights key attributes and features of the storage classes and
“epic use cases” for common usage patterns.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Storage
Type</p></th>
<th class="head"><p>Consumption Model</p></th>
<th class="head"><p>Performance &amp;
Capacity</p></th>
<th class="head"><p>Cost</p></th>
<th class="head"><p>Infrastructure
Strategy</p></th>
<th class="head"><p>Use Case</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Platform
Native</p></td>
<td><p>Managed by the VIM /
Hypervisor and
attached as part of
VNF/CNF start up via
VNF Descriptor,
Volumes shareability
across VNF/CNF
instances is
determined by
platform and storage
capabilities</p></td>
<td><p>Ultra High
Performance &amp; Very
High Performance,
Capacity: 10GB - 5TB,
“Tier 1”</p></td>
<td><p>High to
Very
High</p></td>
<td><p>Always part of VIM
deployment, Storage
is directly next to
vCPU, Can support
highest performance
use cases, Always
available to support
VNF/CNF boot/startup</p></td>
<td><p>Boot/Start VNF/CNF,
Live Migrate Workload
within and across VIMs</p></td>
</tr>
<tr class="row-odd"><td><p>Shared
File
Storage</p></td>
<td><p>Access via Network
File System,
Concurrent
consumption across
multiple VNF/CNFs,
Sharing can be
constrained to
tenancy, cross
tenancy and
externally accessible</p></td>
<td><p>Enterprise
Transactional
Performance (real
time transaction
processing),
Capacity: 5GB -
100TB, Selectable
“Tier 1” to “Tier 3”</p></td>
<td><p>High -
Mid</p></td>
<td><p>Leverage existing
capabilities, Only
build if needed (this
is not needed by many
data plane VNF/CNFs),
If needed for Edge
deployment then aim
to unify with
“Platform Native”
deployment</p></td>
<td><p>VNF/CNF’s able to
share the same file
content</p></td>
</tr>
<tr class="row-even"><td><p>Object
Storage</p></td>
<td><p>Consumed via HTTP/S
restful services,</p></td>
<td><p>Highly distributable
and scalable Provided
by serving
application which
manages storage
needs, Location
Independent</p></td>
<td><p>High to
Mid</p></td>
<td><p>Primarily tenant
application
responsibility</p></td>
<td><p>Cloud Native
Geo-Distributed
VNF/CNFs</p></td>
</tr>
<tr class="row-odd"><td><p>Capacity</p></td>
<td><p>Typically accessed as
per “Shared Storage”
but will likely have
additional storage
stages, Not suitable
for real time
processing</p></td>
<td><p>Very low
transactional
performance, Need
throughput to
accommodate large
data flow, “Tier 3”</p></td>
<td><p>Low</p></td>
<td><p>Use cheapest storage
available that meets
capacity &amp; security
needs</p></td>
<td><p>Archival storage for
tenant/platform
backup/restore, DR</p></td>
</tr>
</tbody>
</table>
<p><strong>Table 3-8:</strong> Tenant Storage Types</p>
<p>In section “3.6.2 Storage Implementation Stereotypes” the General Cloud Storage Model is used to illustrate the
provision of storage. The model can also be used to illustrate the consumption of storage for use by Tenants (see below
for “Platform Native” stereotypes):</p>
<ul class="simple">
<li><p>Platform Native - Hypervisor Attached Consumption Stereotype
(<code class="xref std std-numref docutils literal notranslate"><span class="pre">Platform</span> <span class="pre">Native</span> <span class="pre">-</span> <span class="pre">Hypervisor</span> <span class="pre">Attached</span> <span class="pre">Consumption</span> <span class="pre">Stereotype</span></code>) - where hypervisor consumes Software
Defined Storage via Network (RA-1 - Cinder backend (2)) and the Block Image is attached to Virtual Machine (RAW or
QCOW file within File System), which is used for boot and exposure to virtual machine OS as Block Storage (3). The
virtual machine OS in turn consumes this for use by Tenant Application via File System,</p></li>
<li><p>Platform Native - Container Persistent Consumption Stereotype
(<code class="xref std std-numref docutils literal notranslate"><span class="pre">Platform</span> <span class="pre">Native</span> <span class="pre">-</span> <span class="pre">Container</span> <span class="pre">Persistent</span> <span class="pre">Consumption</span> <span class="pre">Stereotype</span></code>) - is simpler case with Container
Runtime consuming Software Defined Storage (via Reliable Autonomic Distributed Object Store (RADOS) backend (2)) and exposes this to Container as a file system
mount (3).</p></li>
</ul>
<figure class="align-default" id="platform-native-hypervisor-attached-consumption-stereotype">
<img alt="Platform Native - Hypervisor Attached Consumption Stereotype" src="../_images/rm-chap3.6-general-cloud-storage-hypervisor-attached-stereotype-01.png" />
<figcaption>
<p><span class="caption-text">Platform Native - Hypervisor Attached Consumption Stereotype</span><a class="headerlink" href="#platform-native-hypervisor-attached-consumption-stereotype" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="platform-native-container-persistent-consumption-stereotype">
<img alt="Platform Native - Container Persistent Consumption Stereotype" src="../_images/rm-chap3.6-general-cloud-storage-container-persistent-stereotype-01.png" />
<figcaption>
<p><span class="caption-text">Platform Native - Container Persistent Consumption Stereotype</span><a class="headerlink" href="#platform-native-container-persistent-consumption-stereotype" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>Note that a stereotype for Network File Storage consumption is not illustrated as this is simply managed by the Tenant
Application by doing a file systems mount.</p>
<p>In cloud infrastructure, the storage types may manifest in various ways with substantive variations in the architecture
models being used. Examples of this are provided in section <a class="reference internal" href="#storage-implementation-stereotypes">Storage Implementation Stereotypes</a>, with stereotypes
for “Dedicated Storage Appliance” and “Software Defined Storage”. In the consumption case, again there is use of
in-chassis storage to support hypervisor and container host OS/Runtime boot, not for Tenant / User Plane storage
consumption.</p>
</section>
<section id="storage-scenarios-and-architecture-fit">
<h3>Storage Scenarios and Architecture Fit<a class="headerlink" href="#storage-scenarios-and-architecture-fit" title="Link to this heading">¶</a></h3>
<p>The storage model and stereotypical usage scenarios illustrate the key storage uses cases and their applicability to
support storage needs from across a range of cloud deployments. This set of storage uses cases is summarised in the
following tables, including how the stereotypes can support the Anuket Reference Architectures, followed by the key
areas for consideration in such a deployment scenario. The structure of the table is:</p>
<ul class="simple">
<li><p>Use Case - what is the target storage use case being covered (large data-centre, small data-centre, standalone cloud,
edge etc.)</p></li>
<li><p>Stereotype - which of defined stereotypes is used</p></li>
<li><p>Infra / Ctrl / Mgt - is the storage stereotype able to support the:</p>
<ul>
<li><p>Infrastructure - for host computer boot (from either local host storage or PXE),</p></li>
<li><p>Control Plane - for cloud infrastructure control (such as OpenStack (RA1) or Kubernetes (RA2) control functions) and</p></li>
<li><p>Management Plane Needs - for Infrastructure Automation, Tenant VNF/CNF Orchestration and cloud infrastructure
monitoring and assurance</p></li>
</ul>
</li>
<li><p>Tenant / User - is the storage stereotype able to support Tenant / User Plane needs including: Platform Native, Shared
File Storage &amp; Object Storage (as per section <a class="reference internal" href="#storage-for-tenant-consumption">Storage for Tenant Consumption</a>)</p></li>
</ul>
<p>Where:</p>
<ul class="simple">
<li><p>“Y” - Yes and almost always provided</p></li>
<li><p>“O” - Optional and readily accommodated</p></li>
<li><p>“N” - No, not available</p></li>
<li><p>“NA” - Not Applicable for this Use Case / Stereotype</p></li>
</ul>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head" colspan="5"></th>
<th class="head" colspan="7"><p>Tenant / User</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td colspan="2"></td>
<td colspan="3"><p>Infra / Ctrl / Mgt</p></td>
<td colspan="2"><p>Platform Native</p></td>
<td colspan="4"><p>Shared File</p></td>
<td><p>Object</p></td>
</tr>
<tr class="row-odd"><td><p>Use Case</p></td>
<td><p>Stereotype</p></td>
<td><p>Boot</p></td>
<td><p>Ctrl</p></td>
<td><p>Mgt</p></td>
<td><p>Hypervisor
Attached</p></td>
<td><p>Container
Persistent</p></td>
<td><p>Within</p></td>
<td><p>Cross</p></td>
<td><p>Ext</p></td>
<td><p>vNAS</p></td>
<td><p>Object</p></td>
</tr>
<tr class="row-even"><td rowspan="3"><p>Data-centre Storage</p></td>
<td><p>Dedicated Network Storage Appliance</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
</tr>
<tr class="row-odd"><td><p>Dedicated Software Defined Storage</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
</tr>
<tr class="row-even"><td><p>Traditional SAN</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>Satellite data-centre Storage</p></td>
<td><p>Small Software Defined Storage</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
</tr>
<tr class="row-even"><td><p>Small data-centre Storage</p></td>
<td><p>Converged Software Defined Storage</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
</tr>
<tr class="row-odd"><td rowspan="3"><p>Edge Cloud</p></td>
<td><p>Edge Cloud for VNF/CNF Storage</p></td>
<td><p>NA</p></td>
<td><p>O</p></td>
<td><p>NA</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
</tr>
<tr class="row-even"><td><p>Edge Cloud for Apps Storage</p></td>
<td><p>NA</p></td>
<td><p>O</p></td>
<td><p>NA</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-odd"><td><p>Edge Cloud for Content Mgt Storage</p></td>
<td><p>NA</p></td>
<td><p>O</p></td>
<td><p>NA</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>Y</p></td>
</tr>
<tr class="row-even"><td rowspan="3"><p>Split Control/User Plane
Edge Cloud</p></td>
<td><p>Split Edge Ctrl Plane Storage</p></td>
<td><p>NA</p></td>
<td><p>N</p></td>
<td><p>NA</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
</tr>
<tr class="row-odd"><td rowspan="2"><p>Split Edge User Plane Storage</p></td>
<td rowspan="2"><p>NA</p></td>
<td rowspan="2"><p>N</p></td>
<td rowspan="2"><p>NA</p></td>
<td rowspan="2"><p>N</p></td>
<td rowspan="2"><p>N</p></td>
<td rowspan="2"><p>N</p></td>
<td rowspan="2"><p>N</p></td>
<td rowspan="2"><p>N</p></td>
<td rowspan="2"><p>N</p></td>
<td rowspan="2"><p>N</p></td>
</tr>
<tr class="row-even"></tr>
</tbody>
</table>
<p><strong>Table 3-9:</strong> Storage Use Cases and Stereotypes</p>
<p>The storage sub-system is a foundational part of any Cloud Infrastructure, as such it is important to identify the storage needs, based on target tenant use cases,
at inception. This will allow the right set of considerations to be addressed for the deployment. A set of typical considerations is provided:</p>
<ul class="simple">
<li><p>for various use cases to meet functional and performance needs and</p></li>
<li><p>to avoid the need for significant rework of the storage solution and the likely ripple through impact on the broader Cloud Infrastructure.</p></li>
</ul>
<p>The considerations will help to guide the build and deployment of the Storage solution for the various Use Cases and Stereotypes outlined in the summary table.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head" colspan="4"><p>Use Case</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td colspan="4"><p><strong>Data-centre</strong>
<strong>Storage</strong></p></td>
<td><p>Provide a highly reliable and scalable storage capability
that has flexibility to meet diverse needs</p></td>
</tr>
<tr class="row-odd"><td rowspan="3"></td>
<td colspan="3" rowspan="3"><p>Meets Needs of</p></td>
<td><p>Cloud Infrastructure Control Plane (tenant Virtual
Machine and Container life-cycle management and control)</p></td>
</tr>
<tr class="row-even"><td><p>Cloud Infrastructure Management Plane (Cloud
Infrastructure fault and performance management and
platform automation)</p></td>
</tr>
<tr class="row-odd"><td><p>Cloud Infrastructure Tenant / User Plane</p></td>
</tr>
<tr class="row-even"><td></td>
<td colspan="4"><p>General Considerations: What are the general considerations, irrespective of
the deployment stereotype/technology used in the storage sub-system?</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>1</p></td>
<td colspan="3"><p>Can storage support Virtual Machine (RA-1) and Container (RA-2) Hosting
cases from single instance? Noting that if you wish to have single
storage instance providing storage across multiple clusters and/or
availability zones within the same data-centre then this needs to be
factored into the underlay network design.</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>2</p></td>
<td colspan="3"><p>Can the storage system support Live Migration/Multi-Attach within and
across Availability Zones (applicable to Virtual Machine hosting (RA-1))
and how does the Cloud Infrastructure solution support migration of
Virtual Machines between availability zones in general?</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>3</p></td>
<td colspan="3"><p>Can the storage system support the full range of Shared File Storage use
cases: including the ability to control how network exposed Share File
Storage is visible: Within Tenancy, Across Tenancy (noting that a Tenancy
can operate across availability zones) and Externally?</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>4</p></td>
<td colspan="3"><p>Can the storage system support alternate performance tiers to allow
tenant selection of best Cost/Performance option? For very high
performance storage provision, meeting throughput and IOP needs can be
achieved by using: very high IOP flash storage, higher bandwidth
networking,performance optimised replication design and storage pool host
distribution, while achieving very low latency targets require careful
planning of underlay storage VLAN/switch networking.</p></td>
</tr>
<tr class="row-odd"><td></td>
<td colspan="4"><p>Specific Considerations: In selecting a particular stereotype/technology this
can bring with it considerations that are specific to this choice</p></td>
</tr>
<tr class="row-even"><td></td>
<td colspan="4"><p>Dedicated Software Defined Storage</p></td>
</tr>
<tr class="row-odd"><td colspan="2" rowspan="3"></td>
<td><p>1</p></td>
<td colspan="2"><p>Need to establish the physical disk data layout / encoding scheme
choice, options could be: replication / mirroring of data across
multiple storage hosts or CRC-based redundancy management encoding
(such as “erasure encoding”). This typically has performance/cost
implications as replication has a lower performance impact, but
consumes larger number of physical disks. If using replication then
increasing the number of replicas provide greater data loss
prevention, but consumes more disk system backend network bandwidth,
with bandwidth need proportional to number of replicas.</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td colspan="2"><p>In general with Software Defined Storage solution it is not
to use hardware RAID controllers, as this impacts the scope of
recovery on failure as the failed device replacement can only be
managed within the RAID volume that disk is part of. With Software
Defined Storage failure recovering can be managed within the host
that the disk failed in, but also across physical storage hosts.</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td colspan="2"><p>Can storage be consumed optimally irrespective of whether this is at
Control, Management or Tenant / User Plane? Example is iSCSI/NFS,
which while available and providing a common technical capability,
does not provide best achievable performance. Best performance is
achieved using provided OS layer driver that matches the particular
software defined storage implementation (example is using RADOS
driver in Ceph case vs. Ceph ability to expose iSCSI).</p></td>
</tr>
<tr class="row-even"><td></td>
<td colspan="4"><p>Dedicated Network Storage Appliance</p></td>
</tr>
<tr class="row-odd"><td colspan="2"></td>
<td><p>1</p></td>
<td colspan="2"><p>Macro choice is made based on vendor / model selection and
configuration choices available</p></td>
</tr>
<tr class="row-even"><td></td>
<td colspan="4"><p>Traditional SAN</p></td>
</tr>
<tr class="row-odd"><td colspan="2"></td>
<td><p>1</p></td>
<td colspan="2"><p>This is generally made available via Fiber Channel Arbitrated Loop
(FC-AL)/SCSI connectivity and hence has a need for very specific
connectivity. To provide the features required for Cloud
Infrastructure (Shared File Storage, Object Storage and
Multi-tenancy support), a SAN storage systems needs to be augmented
with other gateway/s to provide an IP Network consumable capability.
This is often seen with current deployments where NFS/CIFS (NAS)
Gateway is connected by FC-AL (for storage back-end) and IP Network
for Cloud Infrastructure consumption (front-end). This model helps
to extent use of SAN storage investment. NOTE: This applies to SANs
which use SAS/SATA physical disk devices, as direct connect FC-AL
disk devices are no longer manufactured.</p></td>
</tr>
<tr class="row-even"><td colspan="4"><p><strong>Satellite</strong>
<strong>Data-centre Storage</strong></p></td>
<td><p>Satellite data-centre is a smaller regional  deployment
which has connectivity to and utilises resources
available from the main Data-centre, so only provides
support for subset of needs</p></td>
</tr>
<tr class="row-odd"><td rowspan="7"></td>
<td colspan="3" rowspan="2"><p>Meets Needs of</p></td>
<td><p>Cloud Infrastructure Control Plane (tenant Virtual
Machine and Container life-cycle management and control)</p></td>
</tr>
<tr class="row-even"><td><p>Cloud Infrastructure Tenant/User Plane</p></td>
</tr>
<tr class="row-odd"><td colspan="4"><p>General Considerations: What are the general considerations, irrespective
of the deployment stereotype/technology used in storage sub-system?</p></td>
</tr>
<tr class="row-even"><td><p>1</p></td>
<td colspan="3"><p>Is there a need to support multiple clusters/availability zones at the
same site? If so then use “Data-Centre Storage” use case, otherwise,
consider how to put Virtual Machine &amp; Container Hosting control plane
and Storage control plane on the same set of hosts to reduce footprint.</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td colspan="3"><p>Can Shared File Storage establishment be avoided by using capabilities
provided by large Data-Centre Storage?</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td colspan="3"><p>Can very large capacity storage needs be moved to larger Data-Centre
Storage capabilities?</p></td>
</tr>
<tr class="row-odd"><td colspan="4"><p>Specific Considerations: In selecting a particular stereotype/technology this
can bring with it considerations that are specific to this choice</p></td>
</tr>
<tr class="row-even"><td></td>
<td colspan="4"><p>Small Software Defined Storage</p></td>
</tr>
<tr class="row-odd"><td colspan="2" rowspan="3"></td>
<td><p>1</p></td>
<td colspan="2"><p>Leverage same technology as “Dedicated Software Defined Storage”
scenarios, but avoid/limit Infrastructure boot and Management plane
support and Network Storage support</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td colspan="2"><p>Avoid having dedicated storage instance per cluster/availability
zone</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td colspan="2"><p>Resilience through rapid rebuild (N + 1 failure scenario)</p></td>
</tr>
<tr class="row-even"><td colspan="4"><p><strong>Small Data-centre</strong>
<strong>Storage</strong></p></td>
<td><p>Small data-centre storage deployment is used in cases
where software-defined storage and virtual machine /
container hosting are running on a converged
infrastructure footprint with the aim of reducing the
overall size of the platform. This solution behaves as a
standalone Infrastructure Cloud platform.</p></td>
</tr>
<tr class="row-odd"><td rowspan="8"></td>
<td colspan="3" rowspan="3"><p>Meets Needs of</p></td>
<td><p>Cloud Infrastructure Control Plane (tenant Virtual
Machine and Container life-cycle management and control)</p></td>
</tr>
<tr class="row-even"><td><p>Cloud Infrastructure Management Plane (Cloud
Infrastructure fault and performance management and
platform automation)</p></td>
</tr>
<tr class="row-odd"><td><p>Cloud Infrastructure Tenant / User Plane</p></td>
</tr>
<tr class="row-even"><td colspan="4"><p>General Considerations: What are the general considerations, irrespective of
the deployment stereotype/technology used in the storagesub-system?</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td colspan="3"><p>Is there need to support multiple clusters / availability zones at same
site? See guidance for “Satellite Data-centre Storage” use case(1).</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td colspan="3"><p>Is Shared File Storage required? Check sharing scope carefully as fully
virtualised NFs solution adds complexity and increases resources needs.</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td colspan="3"><p>Is there need for large local capacity? With large capacity flash (15-30
TB/device), the solution can hold significant storage capacity, but need
to consider carefully data loss prevention need and impact on
rebuilt/recovery times.</p></td>
</tr>
<tr class="row-even"><td colspan="4"><p>Specific Considerations: In selecting a particular stereotype/technology this
can bring with it considerations that are specific to this choice</p></td>
</tr>
<tr class="row-odd"><td></td>
<td colspan="4"><p>Converged Software Defined Storage</p></td>
</tr>
<tr class="row-even"><td colspan="2" rowspan="3"></td>
<td><p>1</p></td>
<td colspan="2"><p>Leverage same technology as “Dedicated Software-Defined Storage”
scenarios, but on converged infrastructure. To meet capacity needs
provision three hosts for storage and the rest for virtual
infrastructure and storage control and management and tenant
workload hosting.</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td colspan="2"><p>If the solution needs to host two clusters/availability zones then
have sharable storage instances.</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td colspan="2"><p>Resilience through rapid rebuild (N + 0 or N + 1)</p></td>
</tr>
<tr class="row-odd"><td colspan="4"><p><strong>Edge Cloud for App</strong>
<strong>Storage</strong></p></td>
<td><p>Support the deployment of Applications at the edge, which
tend to have greater storage needs than a network VNF/CNF</p></td>
</tr>
<tr class="row-even"><td rowspan="2"></td>
<td colspan="3" rowspan="2"><p>Meets Needs of</p></td>
<td><p>Cloud Infrastructure Control Plane (tenant Virtual
Machine and Container life-cycle management and control)</p></td>
</tr>
<tr class="row-odd"><td><p>Cloud Infrastructure Tenant / User Plane - very limited
configuration storage</p></td>
</tr>
<tr class="row-even"><td colspan="4"><p><strong>Edge Cloud for</strong>
<strong>VNF/CNF Storage</strong></p></td>
<td><p>Support the deployment of VNF / CNF at the edge.</p></td>
</tr>
<tr class="row-odd"><td rowspan="2"></td>
<td colspan="3" rowspan="2"><p>Meets Needs of</p></td>
<td><p>Cloud Infrastructure Control Plane (tenant Virtual
Machine and Container life-cycle management and control)</p></td>
</tr>
<tr class="row-even"><td><p>Cloud Infrastructure Tenant / User Plane - limited
configuration storage</p></td>
</tr>
<tr class="row-odd"><td colspan="4"><p><strong>Edge Cloud for</strong>
<strong>Content Storage</strong></p></td>
<td><p>Support the deployment of deployment of media content
cache at the edge. This is a very common Content
Distribution Network (CDN) use case</p></td>
</tr>
<tr class="row-even"><td rowspan="7"></td>
<td colspan="3" rowspan="2"><p>Meets Needs of</p></td>
<td><p>Cloud Infrastructure Control Plane (tenant Virtual
Machine and Container life-cycle management and control)</p></td>
</tr>
<tr class="row-odd"><td><p>Cloud Infrastructure Tenant / User Plane - Media Content
storage</p></td>
</tr>
<tr class="row-even"><td colspan="4"><p>General Considerations: What are the general considerations, irrespective of
the deployment stereotype/technology used in the storage sub-system?</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td colspan="3"><p>Consuming and exposing Object storage through Tenant application</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td colspan="3"><p>Use Embedded Shared File Storage for Control and Tenant Storage Needs</p></td>
</tr>
<tr class="row-odd"><td colspan="4"><p>Specific Considerations: In selecting a particular stereotype/technology this
can bring with it considerations that are specific to this choice</p></td>
</tr>
<tr class="row-even"><td colspan="4"><p>Embedded Shared File Storage</p></td>
</tr>
<tr class="row-odd"><td colspan="2"></td>
<td><p>1</p></td>
<td colspan="2"><p>What is the best way to achieve some level of data resilience, while
minimising required infrastructure? (i.e do not have luxury of
having host (VMs) dedicated to supporting storage control and
storage data needs)</p></td>
</tr>
</tbody>
</table>
<p>The General Storage Model illustrates that at the bottom of any storage solution there is always the physical storage
layer and a storage operating system of some sort. In a Cloud Infrastructure environment what is generally consumed is
some form of network storage which can be provided by the:</p>
<ul class="simple">
<li><p>Infrastructure platform underlay network for Control Plan and Platform Native - Hypervisor Attached and Container
Runtime Managed</p></li>
<li><p>Tenant / User overlay network for Shared File Storage and Object Storage</p></li>
</ul>
<p>In general for the provision of storage as shared resource it is not desirable to use “in chassis storage” for anything
other than in the storage devices for platform hypervisor/OS boot or for the hosts providing the storage sub-systems
deployment itself. This is due to difficulty in resulting operational management (see principles section
<a class="reference internal" href="#introduction-to-storage">Introduction to Storage</a> - “Operationally Amenable” above).</p>
<p>For cloud-based storage, “Ephemeral” storage (hypervisor attached or container images which are disposed when VNF/CNF is
stopped) is often distinguished from other persistent storage, however this is a behaviour variation that is managed
via the VNF descriptor rather than a specific Storage Type.</p>
<p>Storage also follows the alignment of separated virtual and physical resources of Virtual Infrastructure Layer and HW
Infrastructure Layer. Reasons for such alignment are described more in Section <a class="reference internal" href="#network">Network</a>.</p>
<p>While there are new storage technologies being made available and a trend towards the use of flash for all physical
storage needs, for the near future, the core storage architecture for Cloud Infrastructure is likely to remain
consistent with the network-based consumption model, as described through the stereotypes.</p>
</section>
</section>
<section id="sample-reference-model-realization">
<h2>Sample reference model realization<a class="headerlink" href="#sample-reference-model-realization" title="Link to this heading">¶</a></h2>
<p>The following diagram presents an example of the realization of the reference model, where a virtual infrastructure
layer contains three coexisting but different types of implementation: a typical IaaS using VMs and a hypervisor for
virtualisation, a CaaS on VM/hypervisor, and a CaaS on bare metal. This diagram is presented for illustration purposes
only and it does not preclude validity of many other different combinations of implementation types. Note that the model
enables several potentially different controllers orchestrating different type of resources (virtual and/or hardware).
Management clients can manage virtual resources via Virtual Infrastructure Manager (Container Infrastructure Service
Manager for CaaS, or Virtual Infrastructure Manager for IaaS), or alternatively hardware infrastructure resources via
hardware infrastructure manager. The latter situation may occur for instance when an orchestrator (an example of a
management client) is involved in provisioning the physical network resources with the assistance of the controllers.
Also, this realization example would enable implementation of a programmable fabric.</p>
<figure class="align-default" id="reference-model-realization-example">
<img alt="Reference model realization example" src="../_images/ch03-model-realization-diagram-2.png" />
<figcaption>
<p><span class="caption-text">Reference model realization example</span><a class="headerlink" href="#reference-model-realization-example" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>The terms Container Infrastructure Service Instance and Container Infrastructure Service Manager should be understood as
defined in ETSI GR NFV-IFA 029 V3.3.1 <span id="id26">[<a class="reference internal" href="chapter01.html#id14" title="Network Functions Virtualisation (NFV) Release 3; Architecture; Report on the Enhancements of the NFV architecture towards &quot;Cloud-native&quot; and &quot;PaaS&quot;. ETSI GR NFV-IFA 029 v3.3.1. URL: https://www.etsi.org/deliver/etsi_gr/NFV-IFA/001_099/029/03.03.01_60/gr_NFV-IFA029v030301p.pdf.">15</a>]</span>. More detailed deployment examples can be found in
<a class="reference internal" href="#network">network</a> of this Reference Model chapter.</p>
</section>
<section id="hardware-acceleration-abstraction">
<h2>Hardware Acceleration Abstraction<a class="headerlink" href="#hardware-acceleration-abstraction" title="Link to this heading">¶</a></h2>
<p>The purpose of a Hardware Accelerator is either to Accelerate the execution of an application or to Offload functions
from the generic CPU to make the application and/or Cloud Infrastructure more efficient from one or more aspects.</p>
<p>Hardware Accelerators are often used in Telco Clouds for many reasons. Some applications require an Hardware Accelerator
to perform tasks that a generic CPU cannot perform fast enough, with enough timing accuracy, or handle the traffic that
must be kept in a single context. Other applications could be satisfied with a generic CPU performance in some
deployment cases, whilst being inefficient in other situations. The Cloud Infrastructure might also benefit from
specialised accelerated HW devices to perform its tasks with less power, space, or cost than a generic CPU.</p>
<p>The Accelerators are specialized resources and generally not expected to exist in large quantities, which makes it
important that these limited HW Accelerators are carefully assigned to where they can be best used most of the time. In
general, this requires that there be software-based alternative functions that can be used for the occasions when HW
Accelerators can not be assigned to accelerate or offload applications or Cloud Infrastructure tasks.</p>
<p>It is preferred that the accelerated or offloaded functions have abstracted interfaces since that would hide the
different implementations from a functional point of view and make orchestrator choices simpler and more transparent to
deploy. It will also allow support for multiple different HW Accelerators, and reducing the operator’s integration and
test efforts of the accelerators and their applications and/or Cloud Infrastructure.</p>
<section id="types-of-accelerators">
<h3>Types of Accelerators<a class="headerlink" href="#types-of-accelerators" title="Link to this heading">¶</a></h3>
<p>Accelerator technologies can be categorized depending on where they are realized in the hardware product and how they
get activated, life cycle managed and supported in running infrastructure.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Acceleration
technology/hardware</p></th>
<th class="head"><p>Example implementation</p></th>
<th class="head"><p>Activation/LCM/support</p></th>
<th class="head"><p>Usage by application tenant</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>CPU instructions</p></td>
<td><p>Within CPU cores</p></td>
<td><p>None for hardware</p></td>
<td><p>Application to load
software library that
recognizes and uses CPU
instructions</p></td>
</tr>
<tr class="row-odd"><td><p>Fixed function accelerator</p></td>
<td><p>Crypto, vRAN-specific
adapter</p></td>
<td><p>Rare updates</p></td>
<td><p>Application to load
software library/driver
that recognizes and uses
the accelerator</p></td>
</tr>
<tr class="row-even"><td><p>Firmware-programmable
adapter</p></td>
<td><p>Network/storage adapter
with programmable part of
firmware image</p></td>
<td><p>Rare updates</p></td>
<td><p>Application normally not
modified or aware</p></td>
</tr>
<tr class="row-odd"><td><p>SmartNIC</p></td>
<td><p>Programmable accelerator
for vSwitch/vRouter, NF
and/or Hardware
Infrastructure</p></td>
<td><p>Programmable by
Infrastructure operator(s)
and/or application
tenant(s)</p></td>
<td><p>3 types/operational modes:
1. Non-programmable
normally with unaware
applications; 2. Once
programmable to activate;
3 Reprogrammable</p></td>
</tr>
<tr class="row-even"><td><p>SmartSwitch-based</p></td>
<td><p>Programmable Switch Fabric
or TOR switch</p></td>
<td><p>Programmable by
Infrastructure operator(s)
and/or application
tenant(s)</p></td>
<td><p>3 operational modes:
1. Non-programmable
normally with unaware
applications; 2. Once
programmable to activate;
3. Reprogrammable</p></td>
</tr>
</tbody>
</table>
<p><strong>Table 3-10:</strong> Hardware acceleration categories, implementation, activation/LCM/support and usage</p>
<figure class="align-default" id="id55">
<img alt="Examples of server- and SmartSwitch-based nodes (for illustration only)" src="../_images/ch03-examples-of-server-and-smartswitch-based-nodes.png" />
<figcaption>
<p><span class="caption-text">Examples of server- and SmartSwitch-based nodes (for illustration only)</span><a class="headerlink" href="#id55" title="Link to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="infrastructure-and-application-level-acceleration">
<h3>Infrastructure and Application Level Acceleration<a class="headerlink" href="#infrastructure-and-application-level-acceleration" title="Link to this heading">¶</a></h3>
<p><code class="xref std std-numref docutils literal notranslate"><span class="pre">Hardware</span> <span class="pre">Acceleration</span> <span class="pre">in</span> <span class="pre">RM</span> <span class="pre">Realization</span> <span class="pre">Diagram</span></code> gives examples for the Hardware Accelerators shown in
<code class="xref std std-numref docutils literal notranslate"><span class="pre">Reference</span> <span class="pre">model</span> <span class="pre">realization</span> <span class="pre">example</span></code>
(the <a class="reference internal" href="#sample-reference-model-realization"><span class="std std-ref">Sample reference model realization</span></a> diagram).</p>
<figure class="align-default" id="hardware-acceleration-in-rm-realization-diagram">
<img alt="Hardware Acceleration in RM Realization Diagram" src="../_images/ch03-hardware-acceleration-in-rm-realization-diagram.png" />
<figcaption>
<p><span class="caption-text">Hardware Acceleration in RM Realization Diagram</span><a class="headerlink" href="#hardware-acceleration-in-rm-realization-diagram" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>Hardware Accelerators are part of the Hardware Infrastructure Layer. Those that need to be activated/programmed will
expose management interfaces and have Accelerator Management software managing them in-band (from host OS) or out of
band (OOB, over some network to the adapter without going through host OS). For more flexibility in management, such
Accelerator Management can be carried over appropriate service with authentication mechanism before being exposed to
Cloud Infrastructure operator and/or Application tenant.</p>
<p>Application uses software library supporting hardware acceleration and running on generic CPU instructions. Mapping
workload to acceleration hardware is done with Cyborg in OpenStack or Device Plugin framework in Kubernetes. Hardware
accelerator supports both in-band and/or out of band management, with service exposing it to Cloud Infrastructure
operator or Application tenant roles.</p>
<p>Hardware Accelerators can be used as:</p>
<ul class="simple">
<li><p>Virtualization Infrastructure layer acceleration: Example can be vSwitch, which can be leveraged agnostically by VNFs
if standard host interfaces (like VirtIO) are used.</p></li>
<li><p>Application layer acceleration: Example of software library/framework (like DPDK) in VM providing Application level
acceleration with (where available) hardware-abstracted APIs to access platform Hardware Acceleration and providing
software equivalent libraries when hardware assist not available.</p></li>
<li><p>Hardware Infrastructure layer offload: Example can be an OOB managed underlay network separation providing network
separation secured from host OS reach on any provisioned transport switch infrastructure.</p></li>
</ul>
<p>Two levels of consumption are for underlay separation or overlay acceleration. Underlay Separation ensures that multiple
different Virtualization Infrastructure instances are kept in separate underlay network access domains. Overlay
Acceleration offloads Virtualization Infrastructure instance vSwitch/vRouter or virtual termination endpoints (for
applications that bypass the Virtual Infrastructure Layer).</p>
<p>Preferably, Application or Infrastructure acceleration can take benefit from underlying hardware acceleration and still
be decoupled from it by using open multi-vendor API for Hardware Acceleration devices like for example:</p>
<ul class="simple">
<li><p>For Linux IO virtualization: VirtIO</p></li>
<li><p>For Network Functions using DPDK libraries: Crypto Device, EthDev, Event Device and Base Band Device</p></li>
<li><p>For O-RAN Network functions: O-RAN Acceleration Abstraction Layer Interface.</p></li>
</ul>
</section>
<section id="example-of-o-ran-acceleration-abstraction-layer-interface">
<h3>Example of O-RAN Acceleration Abstraction Layer Interface<a class="headerlink" href="#example-of-o-ran-acceleration-abstraction-layer-interface" title="Link to this heading">¶</a></h3>
<p>O-RAN Alliance’s Cloudification and Orchestration Workgroup (WG6) defines the Acceleration Abstraction Layer (AAL), an
application-level interface, as the recommended way of decoupling software vendors’ network functions from the different
hardware accelerator implementations.</p>
<figure class="align-default" id="id56">
<img alt="AAL Interface in RM Realization Diagram" src="../_images/ch03-hardware-acceleration-in-rm-realization-diagram_AAL.png" />
<figcaption>
<p><span class="caption-text">AAL Interface in RM Realization Diagram</span><a class="headerlink" href="#id56" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>The document “O-RAN Acceleration Abstraction Layer General Aspects and Principles 1.0” <span id="id27">[<a class="reference internal" href="chapter01.html#id41" title="O-RAN Acceleration Abstraction Layer General Aspects and Principles. O-RAN.WG6.AAL-GAnP-v01.00, November 2020. URL: https://www.o-ran.org.">23</a>]</span>,
and the latest O-RAN WG6 Cloudification and Orchestration Workgroup specifications <span id="id28">[<a class="reference internal" href="chapter01.html#id44" title="WG6: Cloudification and Orchestration Workgroup specfications. URL: https://www.o-ran.org.">24</a>]</span>:</p>
<ul class="simple">
<li><p>Describe the functions conveyed over the AAL interface, including configuration and management functions.</p></li>
<li><p>Identify the requirements as well as general procedures and operations.</p></li>
<li><p>Introduce the initial set of the O-DU/O-CU AAL profiles.</p></li>
</ul>
</section>
<section id="workload-placement">
<h3>Workload Placement<a class="headerlink" href="#workload-placement" title="Link to this heading">¶</a></h3>
<p>Workload placement can be done by a combination of filters/selectors to find appropriate compute resources, subsystems
to manage assignment of scheduled workloads to Hardware Accelerator, and intelligence in the workload to detect the
presence of Hardware Accelerators.</p>
<p>For initial limited cloud deployments of network functions on private clouds it is possible to have a workload placement
orchestrator that handles optimizations of selected virtualisation clusters and available hardware resources. This will
however soon become too complex with the increasing number of acceleration devices, hardware composability and hybrid
multi-cloud deployments.</p>
<p>Growing lists of individual optimizations including hardware acceleration during scheduling makes it more complex to map
workloads to lists of individual optimizations, so such optimizations get grouped together into higher level categories.
An example is having category for real-time and data plane-optimized category instead of specifying individual
optimizations required to reach it.</p>
<p>With further growth in size of clusters and the variety of hardware acceleration, in a hybrid or multi-cloud deployment,
it will be necessary to enable separate optimization levels for the workload placement and each Cloud Infrastructure
provider. The workload placement orchestrator will operate on one or several Cloud Infrastructures resources to satisfy
the workloads according to Service Level Agreements (SLA) that do not specify all implementation and resource details.
Each Cloud Infrastructure provider will make internal Infrastructure optimisations towards their own internal
optimisation targets whilst fulfilling the SLAs.</p>
</section>
<section id="cpu-instructions">
<h3>CPU Instructions<a class="headerlink" href="#cpu-instructions" title="Link to this heading">¶</a></h3>
<p>The CPU architecture often includes instructions and execution blocks for most common compute-heavy algorithms like
block cypher (example AES-NI), Random Number Generator or vector instructions. These functions are normally consumed in
infrastructure software or applications by using enabled software libraries that run faster when custom CPU instructions
for the execution of such functions are available in hardware and slower when these specific instructions are not
available in hardware as only the general CPU instructions are used. Custom CPU instructions do not need to be activated
or life-cycle-managed. When scheduling workloads, compute nodes with such custom CPU instructions can be found by
applications or an orchestrator using OpenStack Nova filters or Kubernetes Node Feature Discovery labels, or directly
from the Hardware Management layer.</p>
</section>
<section id="fixed-function-accelerators">
<h3>Fixed Function Accelerators<a class="headerlink" href="#fixed-function-accelerators" title="Link to this heading">¶</a></h3>
<p>Fixed function accelerators can come as adapters with in-line (typically PCIe adapter with Ethernet ports or storage
drives) or look-aside (typically PCIe adapters without any external ports) functionality, additional chip on
motherboard, included into server chipsets or packaged/embedded into main CPU. They can accelerate cryptographic
functions, highly parallelized or other specific algorithms. Initial activation and rare life cycle management events
(like updating firmware image) can typically be done from the Host OS (e.g. the OS driver or a Library), the Hardware
Infrastructure Manager (from a library) or the NF (mostly through a library).</p>
<p>Beyond finding such compute nodes during scheduling workloads, those workloads also need to be mapped to the
accelerator, both of which in Kubernetes can be done with Device Plugin framework. Once mapped to the application, the
application can use enabled software libraries and/or device drivers that will use hardware acceleration. If hardware
acceleration is used to improve cost/performance, then application can also run on generic compute node without hardware
accelerator when application will use the same software library to run on generic CPU instructions.</p>
</section>
<section id="firmware-programmable-adapters">
<h3>Firmware-programmable Adapters<a class="headerlink" href="#firmware-programmable-adapters" title="Link to this heading">¶</a></h3>
<p>Firmware-programmable network adapters with programmable pipeline are types of network adapters where usual Ethernet
controller functionality (accelerates common network overlays, checksums or protocol termination) can be extended with
partially programmable modules so that additional protocols can be recognized, parsed and put into specific queues,
which helps increase performance and reduce load on main CPU.</p>
<p>Firmware-programmable storage adapters can offload some of the storage functionality and include storage drive emulation
to enable partial drive assignments up to the accessing host OS. These adapters can over time include more supported
storage offload functions or support more drive emulation functions.</p>
<p>Before being used, such adapters have to be activated by loading programmable module that typically accelerates the
Virtualization Infrastructure, so it is not often reprogrammed. Doing this in multivendor environments can lead to
complexities because the adapter hardware is typically specified, installed and supported by server vendor while the
programmable image on the adapter is managed by SDN, Storage Controller or Software Infrastructure vendor.</p>
</section>
<section id="smartnics">
<h3>SmartNICs<a class="headerlink" href="#smartnics" title="Link to this heading">¶</a></h3>
<p>Programmable SmartNIC accelerators can come as programmable in-line adapters (typically PCIe adapter with Ethernet
ports), or network connected pooled accelerators like farms of GPU or FPGA where the normal CPU PCIe connection is
extended with an Ethernet hop.</p>
<p>There are two main types of Smart NICs that can accelerate network functions in-line between CPU and Ethernet ports of
servers. The simpler types have a configurable or programmable packet pipeline that can implement offload for the
infrastructure virtual switching or part of an application functions data plane. The more advanced type, often called
Data Processing Unit (DPU), have a programmable pipeline and some strong CPU cores that simultaneously can implement
underlay networking separation and trusted forwarding functions, infrastructure virtual switching data and control plane
as well as part of an application functions control plane.</p>
<figure class="align-default" id="id57">
<img alt="Example SmartNIC Deployment Model That Accelerates Two Workloads and Has OOB Management" src="../_images/ch03-example-smartnic-deployment-model.png" />
<figcaption>
<p><span class="caption-text">Example SmartNIC Deployment Model That Accelerates Two Workloads and Has OOB Management</span><a class="headerlink" href="#id57" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<section id="simple-smartnic">
<h4>Simple SmartNIC<a class="headerlink" href="#simple-smartnic" title="Link to this heading">¶</a></h4>
<p>The preferred usage of a simple SmartNIC is for the Virtualization Infrastructure usage that typically implements the
data (forwarding) plane of the virtual switch or router. These deployments can offer a standardized higher-level
abstract interface towards the application tenants such as VirtIO that supports good portability and is by that the
preferred usage method.</p>
<p>Simple SmartNICs direct usage by the application tenant (VNF or CNF), where it acts as a dedicated accelerator
appliance, require the application tenant to manage loading and the function that is loaded in the SmartNIC as well as
any interface to the offloaded network functions. Such deployment is similar to the NIC PCI Pass-Through in that it
bypasses the Virtualization Infrastructure layer’s virtual switching, which require all network encapsulation, mapping
and separation to be done by the underlay network, often by manual provisioning and therefore is not a preferred usage
method.</p>
</section>
<section id="dpu">
<h4>DPU<a class="headerlink" href="#dpu" title="Link to this heading">¶</a></h4>
<p>The DPU can accelerate software infrastructure functions (vSwitch/vRouter) from the main CPU and simultaneously offer
networking services e.g. load balancers, firewalls and application tenant offload functions. Through Out of band
management it can also ensure underlay separation and map a selected part of the underlay network to the specific
Virtualization Infrastructure instance that the server it is mounted on requires allowing them to be used on any
statically provisioned underlay network.</p>
<p>The forwarding path (data plane) needs to be installed and controlled by the Hardware Infrastructure Manager through an
isolated Out of band management channel into the DPU control and operating system completely out of reach for the main
CPU Host SW. All content in the forwarding path must come from Hardware Infrastructure operator trusted code since any
fault or malicious content can seriously disturb the whole network for all connected devices.</p>
<p>The trusted forwarding functions must be handled through a Hardware Infrastructure Management repository and have APIs
for their respective control functions. These APIs must have an ability to handle some version differences since the
forwarding and control planes life cycle management will not be atomic. The offload functions that should be offered as
services must have published and preferably standardized open APIs, but the application specific forwarding functions do
not have to be open APIs since they will only communicate with the application tenant provided control functions.
<a href="#id58"><span class="problematic" id="id71">`P4`__</span></a> <span id="id29">[]</span> and <a href="#id58"><span class="problematic" id="id72">`OpenConfig`__</span></a> <span id="id30">[]</span> are examples of suitable languages and models,
with different levels of flexibility, usable for these forwarding and control functions.</p>
<p>The separated management channel could either come in through the BMC, a direct management port on the DPU or through a
management VPN on the switch ports. This enable the Hardware Infrastructure Management to automate its networking
through the DPU without any need to dynamically manage the switch fabric, thereby enabling a free choice of switch
fabric vendor. These deployments allow the switch fabric to be statically provisioned by the operators networking
operation unit, as it is often required.</p>
<p>The DPU can offload control and data plane of the virtual switching to the DPU as well as trusted hardware offload for
virtualized Packet Core and Radio data plane networking and transport related functionality in a power efficient way. It
can also offload relevant application tenant control functions if the DPU offers an Execution Environment for VMs or
containers and there is space and performance headroom. In such cases the DPU must also setup a communication channel
into respective application tenant environment.</p>
</section>
</section>
<section id="smart-switches">
<h3>Smart Switches<a class="headerlink" href="#smart-switches" title="Link to this heading">¶</a></h3>
<p>Smart Switches can be broadly categorized into Configurable Switches and Programmable Switches.</p>
<p>Configurable Smart Switches run generic “smart” configurable network operating system offering full range of network
functionality and are flexible enough to support most network solutions. The most common such network operating system
is Linux-based <a href="#id58"><span class="problematic" id="id73">`SONiC`__</span></a> <span id="id31">[]</span> allowing hardware and software disaggregation by running
on switches from multiple switch vendors with different types of vendor fixed-function ASICs. Still, SONiC today cannot
implement new type of data plane functionality or patch/modify/correct an ASIC, which is the type of support offered by
programmable smart switches.</p>
<p>Programmable Smart Switches make it possible to quickly support new or correct/modify existing protocols and network
functions, allow end customers to implement network functions, and to only implement and load functionality that is
needed. Such switches contain one or more programmable switch ASICs of the same or different types. The two most used
programming languages are <a href="#id58"><span class="problematic" id="id74">`P4`__</span></a> <span id="id32">[]</span> and <a href="#id58"><span class="problematic" id="id75">`NPL`__</span></a> <span id="id33">[]</span>, and both can be used with
vendor-specific toolchains to program their switch ASICs and/or FPGAs. Open Networking Foundation
<a href="#id58"><span class="problematic" id="id76">`Stratum`__</span></a> <span id="id34">[]</span> is an example of network operating system that offers generic life
cycle management control services for the P4 components and a management API. The control API for the individual network
functions are not part of the Stratum APIs.</p>
<p>Based on Smart Switches, products exist for fully integrated edge and fabric solutions from vendors like Arista, Cisco
or Kaloom.</p>
</section>
<section id="decoupling-applications-from-infrastructure-and-platform-with-hardware-acceleration">
<h3>Decoupling Applications from Infrastructure and Platform with Hardware Acceleration<a class="headerlink" href="#decoupling-applications-from-infrastructure-and-platform-with-hardware-acceleration" title="Link to this heading">¶</a></h3>
<p>Decoupling <a class="reference external" href="https://cntt.readthedocs.io/en/latest/common/glossary.html#cloud-platform-abstraction-related-terminology" title="(in Anuket Specifications)"><span>Cloud Platform Abstraction Related Terminology:</span></a> applications from hardware accelerator
is normally accomplished using drivers that, if available, are preferred with standardised interfaces across vendors and
their products, or if not available then through drivers specific to the vendor hardware device. Decoupling
infrastructure software from hardware accelerators is also preferred using standard interfaces. If those are not
available for target hardware accelerator, coupling one or limited number of software infrastructures is less of an
issue compared to coupling multiple applications.</p>
<p>Taking advantage of RM and RA environments with common capabilities, applications can be developed and deployed more
rapidly, providing more service agility and easier operations. The extent to which this can be achieved will depend on
levels of decoupling between application and infrastructure or platform underneath the application:</p>
<section id="infrastructure">
<h4>Infrastructure:<a class="headerlink" href="#infrastructure" title="Link to this heading">¶</a></h4>
<ul class="simple">
<li><dl class="simple">
<dt>Application functionality or application control requires infrastructure components beyond RM profiles or</dt><dd><p>infrastructure configuration changes beyond APIs specified by RA. Generally, such an application is tightly coupled
with the infrastructure which results in an Appliance deployment model (see
<a class="reference external" href="https://cntt.readthedocs.io/en/latest/common/glossary.html#cloud-platform-abstraction-related-terminology" title="(in Anuket Specifications)"><span>Cloud Platform Abstraction Related Terminology:</span></a>).</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Application control using APIs specified by RA finds nodes (already configured in support of the profiles) with</dt><dd><p>the required infrastructure component(s), and in that node using APIs specified by RA configures infrastructure
components that make application work. Example is an application that to achieve latency requirements needs
certain hardware acceleration available in RM profile and is exposed through APIs specified by RA.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Application control using APIs specified by RA finds nodes (already configured in support of the profiles) with</dt><dd><p>optional infrastructure component(s), and in these nodes using APIs specified by RA configures infrastructure
component(s) that make application work better (like more performant) than without that infrastructure component.
Example is an application that would have better cost/performance with certain acceleration adapter but can also
work without it.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Application control using APIs specified by RA finds general profile nodes without any specific infrastructure</dt><dd><p>components.</p>
</dd>
</dl>
</li>
</ul>
</section>
<section id="platform-services">
<h4>Platform Services:<a class="headerlink" href="#platform-services" title="Link to this heading">¶</a></h4>
<ul class="simple">
<li><dl class="simple">
<dt>Application functionality or application control can work only with its own components instead of using defined</dt><dd><p>Platform Services. Example is an application that brings its own Load Balancer.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>With custom integration effort, application can be made to use defined Platform Services. Example is application</dt><dd><p>that with custom integration effort can use defined Load Balancer which can be accelerated with hardware
acceleration in way that is fully decoupled from application (i.e. application does not have awareness of Load
Balancer being hardware-accelerated).</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Application is designed and can be configured for running with defined Platform Services. Example is application</dt><dd><p>that can be configured to use defined Load Balancer which can be accelerated with hardware acceleration.</p>
</dd>
</dl>
</li>
</ul>
</section>
</section>
</section>
<section id="address-family-for-xdp-af-xdp">
<h2>Address Family For XDP (AF_XDP)<a class="headerlink" href="#address-family-for-xdp-af-xdp" title="Link to this heading">¶</a></h2>
<p><a href="#id58"><span class="problematic" id="id77">`Address Family For XDP (AF_XDP)`__</span></a> <span id="id35">[]</span> is optimized for
high performance packet processing and introduced in Linux kernel v4.18. This new socket type leverages the <a href="#id58"><span class="problematic" id="id78">`eXpress
Data Path (XDP)`__</span></a> <span id="id36">[]</span> in-kernel fast-path to transfer
traffic frames from the NIC driver directly to userspace without the need for full network stack. XDP is an <a href="#id58"><span class="problematic" id="id79">`Extended
Berkley Packet Filter`__</span></a> <span id="id37">[]</span> (eBPF) software program.</p>
<p>By using the XDP_REDIRECT action from that XDP program, ingress frames can be redirected to other XDP-enabled network
devices. The fastest working mode of operation is Zero-Copy mode in enabled XDP drivers.</p>
<figure class="align-default" id="af-xdp-architecture">
<img alt="AF_XDP Architecture" src="../_images/ch03-afxdp-arch.png" />
<figcaption>
<p><span class="caption-text">AF_XDP Architecture</span><a class="headerlink" href="#af-xdp-architecture" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>Linux-native applications can open an AF_XDP socket to receive raw packets directly from the NIC, by using <a href="#id58"><span class="problematic" id="id80">`libbpf`__</span></a> <a href="#id38"><span class="problematic" id="id39">:cite:p:`libbpf
` library functions to register a packet buffer area
where packets will be located, and to create and bind the socket to a networking interface. DPDK-based applications
can use `AF_XDP Poll Mode Driver`__</span></a> <span id="id40">[]</span>. VPP-based applications can use
<a href="#id58"><span class="problematic" id="id81">`AF_XDP Device Driver`__</span></a> <span id="id41">[]</span>.</p>
<p>In virtualized environments AF_XDP could be used as interface between guest Kernel and user space application, but still need SR-IOV or virtio to get traffic to the VM.</p>
</section>
<section id="energy-efficiency">
<h2>Energy efficiency<a class="headerlink" href="#energy-efficiency" title="Link to this heading">¶</a></h2>
<p>Energy efficiency should be an overall requirement for the cloud infrastructure itself, the workloads
hosted by this infrastructure, and the interface layer between them.</p>
<p>For telecommunication networks, energy efficiency is defined by <a href="#id58"><span class="problematic" id="id82">`ITU-T L.1330`__</span></a> <span id="id42">[<a class="reference internal" href="chapter01.html#id45" title="Energy efficiency measurement and metrics for telecommunication networks. ITU-T L.1330. URL: https://www.itu.int/rec/T-REC-L.1330.">25</a>]</span>
as “the relation between the useful output and energy consumption”, the useful output being a metric which represents the capacity provided by the service whose energy efficiency is
assessed. As an example, the useful output of a traffic forwarding function can be the data volume forwarded (e.g., measured
in Byte) and the assessment of its energy efficiency is then based on the ratio between this volume and the energy
consumed for processing it (e.g., measured in Watt.hour) :
Energy Efficicency (B/Wh) = Traffic Volume / Consumed Energy.</p>
<p>As elaborated in the Next Generation Alliance’s whitepaper <a href="#id58"><span class="problematic" id="id83">`NGA Green G`__</span></a> <span id="id43">[<a class="reference internal" href="chapter01.html#id52" title="Next G Alliance Green G: The path towards sustainable 6G. URL: https://nextgalliance.org/white_papers/green-g-the-path-towards-sustainable-6g/.">26</a>]</span>,
with the global migration from 4G to 5G, one can observe the rise in data center power consumption with the parallel reduction in the energy consumption of core network
elements. This observation emphasizes the importance of the energy efficiency on the infrastructure and workload levels, and on the interface layer between them.</p>
<p>Examples of opportunities for the energy demand and cost reductions for the telecommunications operators are classified below.</p>
<ul class="simple">
<li><p>Optimisation based on workload load demand, enhanced by AI-based smart monitoring
- Smart sleep and shutdowns of elements of infrastructure
- Adaptive power consumption
- Time of usage optimisation
- Multiple input and output muting
- Cross base-station optimisation</p></li>
<li><p>Technology advancements based optimisation
- 2G/3G legacy shutdowns
- 3G/4G/5G optimisation
- Cooling optimisation
- Improved insulation</p></li>
<li><p>Energy source
- Sustainable energy generation
- Procurement of green energy
- Fuel usage optimisation</p></li>
</ul>
<p>In this Reference Model, the focus is on the first group, which is related to the optimisation on the workload to infrastructure interface level.</p>
<p>The method for assessing energy efficiency depends on the service targeted and the objectives. For NFV, ETSI proposes
a method for production environment in <a href="#id58"><span class="problematic" id="id84">`ETSI EN 303 471`__</span></a> <span id="id44">[<a class="reference internal" href="chapter01.html#id46" title="Energy Efficiency measurement methodology and KPI/metrics for NFV. ETSI EN 303 471. URL: https://portal.etsi.org/webapp/workprogram/Report_WorkItem.asp?WKI_ID=50095.">27</a>]</span> and another one for laboratory one
in <a href="#id58"><span class="problematic" id="id85">`ETSI ES 203 539`__</span></a> <span id="id45">[<a class="reference internal" href="chapter01.html#id47" title="Measurement method for energy efficiency of Network Functions Virtualisation (NFV) in laboratory environment. ETSI ES 203 539. URL: https://portal.etsi.org/webapp/workprogram/Report_WorkItem.asp?WKI_ID=47210.">28</a>]</span> (which is
a common work with ITU-T which published as <a href="#id58"><span class="problematic" id="id86">`ITU-T L.1361`__</span></a> <span id="id46">[<a class="reference internal" href="chapter01.html#id48" title="Measurement method for energy efficiency of network functions virtualization. ITU-T L.1361. URL: https://www.itu.int/rec/T-REC-L.1361.">29</a>]</span>).</p>
<p>Whatever the method and the service, it requires the cloud infrastructure to provide some <strong>energy consumption metrics</strong>
for different parts of the infrastructure hardware (server, CPU etc.) as included in
<a class="reference internal" href="chapter04.html#internal-performance-measurement-capabilities"><span class="std std-ref">Internal Performance Measurement Capabilities</span></a>.
These metrics can be an amount of consumed energy (measured in Joule or Watt.hour) or a real-time power utilisation
(measured in Watt or Joule/second) as proposed by <a href="#id58"><span class="problematic" id="id87">`DMTF Redfish DSP0268 2022.2`__</span></a> <span id="id47">[<a class="reference internal" href="chapter01.html#id21" title="DMTF RedFish Specification. URL: https://www.dmtf.org/sites/default/files/standards/documents/DSP0268_2022.2.pdf.">30</a>]</span> which specifies metrics
EnergykWh and PowerWatts for this purpose.</p>
<p>Some relevant information regarding NFV energy efficiency can also be found in <a href="#id58"><span class="problematic" id="id88">`Open RAN Technical Priority - Focus
on Energy Efficiency (March 2022)`__</span></a> <span id="id48">[<a class="reference internal" href="chapter01.html#id49" title="Open RAN Technical Priority - Focus on Energy Efficiency. March 2022. URL: https://www.o-ran.org/ecosystem-resources.">31</a>]</span> and <a href="#id58"><span class="problematic" id="id89">`QuEST Forum - NFV
Workload Efficiency Whitepaper (October 2016)`__</span></a> <span id="id49">[<a class="reference internal" href="chapter01.html#id50" title="QuEST Forum - NFV Workload Efficiency Whitepaper. October 2016. URL: https://tl9000.org/resources/documents/NFV%20Workload%20Efficiency%20Whitepaper.pdf.">32</a>]</span>.</p>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
    
        <div id="show_right_sidebar">
            <p><a class="toggle_right_sidebar" href="#"><span class="icon">&lt;</span><span>Page contents</span></a></p>
        </div>

        <div id="right_sidebar">
            <p><a class="toggle_right_sidebar" href="#"><span class="icon">&gt;</span><span>Page contents:</span></a></p>
            <div class="page_toc">
                <ul>
<li><a class="reference internal" href="#">Modelling</a><ul>
<li><a class="reference internal" href="#model">Model</a></li>
<li><a class="reference internal" href="#virtual-infrastructure-layer">Virtual Infrastructure Layer</a><ul>
<li><a class="reference internal" href="#virtual-resources">Virtual Resources</a><ul>
<li><a class="reference internal" href="#tenant">Tenant</a></li>
<li><a class="reference internal" href="#virtual-compute">Virtual Compute</a></li>
<li><a class="reference internal" href="#virtual-storage">Virtual Storage</a></li>
<li><a class="reference internal" href="#virtual-network">Virtual Network</a></li>
<li><a class="reference internal" href="#availability-zone">Availability Zone</a></li>
</ul>
</li>
<li><a class="reference internal" href="#virtual-infrastructure-manager">Virtual Infrastructure Manager</a></li>
</ul>
</li>
<li><a class="reference internal" href="#hardware-infrastructure-layer">Hardware Infrastructure Layer</a><ul>
<li><a class="reference internal" href="#hardware-infrastructure-resources">Hardware Infrastructure Resources</a><ul>
<li><a class="reference internal" href="#hardware-acceleration-resources">Hardware Acceleration Resources</a></li>
</ul>
</li>
<li><a class="reference internal" href="#hardware-infrastructure-manager">Hardware Infrastructure Manager</a></li>
</ul>
</li>
<li><a class="reference internal" href="#the-redfish-standard-based-hardware-infrastructure-manager">The Redfish® Standard based Hardware Infrastructure Manager</a></li>
<li><a class="reference internal" href="#how-redfish-fits-into-the-etsi-nfv-networking-reference-model">How Redfish fits into the ETSI NFV Networking Reference Model</a></li>
<li><a class="reference internal" href="#network">Network</a><ul>
<li><a class="reference internal" href="#network-principles">Network Principles</a></li>
<li><a class="reference internal" href="#network-layering-and-concepts">Network Layering and Concepts</a><ul>
<li><a class="reference internal" href="#underlay-and-overlay-networking-concepts">Underlay and Overlay Networking Concepts</a></li>
<li><a class="reference internal" href="#hardware-and-virtual-infrastructure-layer-concepts">Hardware and Virtual Infrastructure Layer Concepts</a></li>
<li><a class="reference internal" href="#software-defined-underlay-and-overlay-networking-concepts">Software Defined Underlay and Overlay Networking Concepts</a></li>
<li><a class="reference internal" href="#programmable-networking-fabric-concept">Programmable Networking Fabric Concept</a></li>
</ul>
</li>
<li><a class="reference internal" href="#networking-reference-model">Networking Reference Model</a></li>
<li><a class="reference internal" href="#deployment-examples-based-on-the-networking-reference-model">Deployment Examples Based on the Networking Reference Model</a><ul>
<li><a class="reference internal" href="#switch-fabric-and-smartnic-examples-for-underlay-networking-separation">Switch Fabric and SmartNIC Examples For Underlay Networking Separation</a></li>
<li><a class="reference internal" href="#sdn-overlay-and-sdn-underlay-layering-and-relationship-example">SDN Overlay and SDN Underlay layering and relationship example</a></li>
<li><a class="reference internal" href="#example-of-iaas-and-caas-virtualization-infrastructure-instances-on-a-shared-hw-infrastructure-with-sdn">Example of IaaS and CaaS Virtualization Infrastructure Instances on a Shared HW Infrastructure With SDN</a></li>
</ul>
</li>
<li><a class="reference internal" href="#service-function-chaining">Service Function Chaining</a><ul>
<li><a class="reference internal" href="#service-function-chaining-model-introduction">Service Function Chaining Model Introduction</a></li>
<li><a class="reference internal" href="#sfc-architecture">SFC Architecture</a></li>
<li><a class="reference internal" href="#information-flows-in-service-function-chaining">Information Flows in Service Function Chaining</a><ul>
<li><a class="reference internal" href="#creation-of-service-function-chain">Creation of Service Function Chain</a></li>
<li><a class="reference internal" href="#updating-service-function-chain">Updating Service Function Chain</a></li>
<li><a class="reference internal" href="#data-steering-in-service-function-chain">Data Steering in Service Function Chain</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#time-sensitive-networking">Time Sensitive Networking</a></li>
<li><a class="reference internal" href="#load-balancer">Load Balancer</a></li>
<li><a class="reference internal" href="#kubernetes-networking-semantics">Kubernetes Networking Semantics</a><ul>
<li><a class="reference internal" href="#built-in-kubernetes-network-functionality">Built in Kubernetes Network Functionality</a></li>
<li><a class="reference internal" href="#multiple-networks-and-advanced-configurations">Multiple Networks and Advanced Configurations</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#storage">Storage</a><ul>
<li><a class="reference internal" href="#introduction-to-storage">Introduction to Storage</a></li>
<li><a class="reference internal" href="#storage-implementation-stereotypes">Storage Implementation Stereotypes</a></li>
<li><a class="reference internal" href="#storage-for-tenant-consumption">Storage for Tenant Consumption</a></li>
<li><a class="reference internal" href="#storage-scenarios-and-architecture-fit">Storage Scenarios and Architecture Fit</a></li>
</ul>
</li>
<li><a class="reference internal" href="#sample-reference-model-realization">Sample reference model realization</a></li>
<li><a class="reference internal" href="#hardware-acceleration-abstraction">Hardware Acceleration Abstraction</a><ul>
<li><a class="reference internal" href="#types-of-accelerators">Types of Accelerators</a></li>
<li><a class="reference internal" href="#infrastructure-and-application-level-acceleration">Infrastructure and Application Level Acceleration</a></li>
<li><a class="reference internal" href="#example-of-o-ran-acceleration-abstraction-layer-interface">Example of O-RAN Acceleration Abstraction Layer Interface</a></li>
<li><a class="reference internal" href="#workload-placement">Workload Placement</a></li>
<li><a class="reference internal" href="#cpu-instructions">CPU Instructions</a></li>
<li><a class="reference internal" href="#fixed-function-accelerators">Fixed Function Accelerators</a></li>
<li><a class="reference internal" href="#firmware-programmable-adapters">Firmware-programmable Adapters</a></li>
<li><a class="reference internal" href="#smartnics">SmartNICs</a><ul>
<li><a class="reference internal" href="#simple-smartnic">Simple SmartNIC</a></li>
<li><a class="reference internal" href="#dpu">DPU</a></li>
</ul>
</li>
<li><a class="reference internal" href="#smart-switches">Smart Switches</a></li>
<li><a class="reference internal" href="#decoupling-applications-from-infrastructure-and-platform-with-hardware-acceleration">Decoupling Applications from Infrastructure and Platform with Hardware Acceleration</a><ul>
<li><a class="reference internal" href="#infrastructure">Infrastructure:</a></li>
<li><a class="reference internal" href="#platform-services">Platform Services:</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#address-family-for-xdp-af-xdp">Address Family For XDP (AF_XDP)</a></li>
<li><a class="reference internal" href="#energy-efficiency">Energy efficiency</a></li>
</ul>
</li>
</ul>

            </div>
        </div>
    

      <div class="clearer"></div>
    </div>
    <div class="button_nav_wrapper">
        <div class="button_nav">
            <div class="left">
                
            </div>

            <div class="right">
                
            </div>
        </div>
    </div>


    <div class="footer" role="contentinfo">
    &#169; Copyright 2022, Anuket. Licensed under CC BY 4.0.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    </div>

<p id="theme_credit">Styled using the <a href="https://github.com/piccolo-orm/piccolo_theme">Piccolo Theme</a></p>
  </body>
</html>